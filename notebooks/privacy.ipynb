{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f079de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa49509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_real = pd.read_csv('real/static_numerical.csv')\n",
    "sc_real = pd.read_csv('real/static_categorical.csv')\n",
    "tn_real = pd.read_csv('temporal_numerical.csv')\n",
    "tc_real = pd.read_csv('temporal_categorical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d42547",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_gen = pd.read_csv('generated/static_numerical.csv')\n",
    "sc_gen = pd.read_csv('generated/static_categorical.csv')\n",
    "tn_gen = pd.read_csv('generated/temporal_numerical.csv')\n",
    "tc_gen = pd.read_csv('generated/temporal_categorical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_wide_format(long_df, time_col='TIME'):\n",
    "    long_df = long_df.sort_index()\n",
    "    \n",
    "    time_diff = long_df[time_col].diff()\n",
    "    \n",
    "    new_patient_marker = (time_diff <= 0).astype(int)\n",
    "    \n",
    "    patient_id_col = new_patient_marker.cumsum()\n",
    "    \n",
    "    df_with_ids = long_df.copy()\n",
    "    df_with_ids['patient_id'] = patient_id_col\n",
    "    \n",
    "    print(\"Aggregating data... this may take a moment.\")\n",
    "    wide_df = df_with_ids.groupby('patient_id').agg(list)\n",
    "    \n",
    "    if 'patient_id' in wide_df.columns:\n",
    "        wide_df = wide_df.drop(columns=['patient_id'])\n",
    "        \n",
    "    print(\"Conversion complete.\")\n",
    "    return wide_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0766b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_gen_new = convert_to_wide_format(tn_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba35fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_gen_new = convert_to_wide_format(tc_gen,time_col='DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7a2ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tc_gen_new.shape)\n",
    "print(tn_gen_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7875bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_list_eval(s):\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    if isinstance(s, (list, np.ndarray)):\n",
    "        return s \n",
    "\n",
    "    if not isinstance(s, str) or not s.startswith('['):\n",
    "        return [s] \n",
    "\n",
    "    try:\n",
    "        s_safe = s.replace('nan', 'None') \n",
    "        evaluated_list = ast.literal_eval(s_safe)\n",
    "        evaluated_list = [np.nan if x is None else x for x in evaluated_list]\n",
    "        return evaluated_list\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "def fix_dataframe_dtypes(df):\n",
    "    print(f\"Fixing dtypes for {df.shape[0]} rows...\")\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].apply(safe_list_eval)\n",
    "            \n",
    "    print(\"Dtype conversion complete.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b2e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_real_m  = fix_dataframe_dtypes(tn_real)\n",
    "tc_real_m = fix_dataframe_dtypes(tc_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686ecd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataframes(temp_num_df, temp_cat_df, static_num_df, static_cat_df):\n",
    "    \n",
    "    temporal_combined = temp_num_df.join(temp_cat_df, how='outer')\n",
    "    static_combined = static_num_df.join(static_cat_df, how='outer')\n",
    "    \n",
    "    static_cols = static_combined.columns\n",
    "    for col in static_cols:\n",
    "        static_combined[col] = static_combined[col].apply(\n",
    "            lambda x: [x] if pd.notna(x) else []\n",
    "        )\n",
    "            \n",
    "    final_wide_df = temporal_combined.join(static_combined, how='outer')\n",
    "    \n",
    "    for col in final_wide_df.columns:\n",
    "        final_wide_df[col] = final_wide_df[col].apply(\n",
    "            lambda x: x if isinstance(x, list) else []\n",
    "        )\n",
    "            \n",
    "    return final_wide_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab83427",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_real = combine_dataframes(tn_real_m[tn_real.columns[1:]],tc_real_m[tc_real.columns[1:]],sn_real,sc_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dedf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_real.drop(columns=['HIV status'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cfc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_real.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32ef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = tc_gen_new.shape[0]\n",
    "tn_gen_m = tn_gen_new[:num]\n",
    "tc_gen_m = tc_gen_new[:num]\n",
    "sn_gen_m = sn_gen[:num]\n",
    "sc_gen_m = sc_gen[:num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f72bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gen = combine_dataframes(tn_gen_m[tn_gen.columns[1:]],tc_gen_m[tc_real.columns[1:]],sn_gen_m,sc_gen_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04916626",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_gen.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b7d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_real.to_pickle('real_wide.pkl')\n",
    "final_gen.to_pickle('generated_wide.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "class PrivacyEvaluator:\n",
    "    def __init__(self, real_train_df, real_test_df, synthetic_df, \n",
    "                 static_num_cols, static_cat_cols, \n",
    "                 temp_num_cols, temp_cat_cols, \n",
    "                 max_seq_len=None, n_samples=None, n_components_pca=None):\n",
    "        \n",
    "        self.static_num_cols_ = static_num_cols\n",
    "        self.static_cat_cols_ = static_cat_cols\n",
    "        self.temp_num_cols_ = temp_num_cols\n",
    "        self.temp_cat_cols_ = temp_cat_cols\n",
    "        self.use_pca_ = n_components_pca is not None\n",
    "        \n",
    "        self.all_cat_cols_ = list(static_cat_cols) + list(temp_cat_cols)\n",
    "        \n",
    "        if n_samples:\n",
    "            print(f\"Sampling data down to {n_samples} patients.\")\n",
    "            real_train_df = real_train_df.sample(n=min(n_samples, len(real_train_df)), random_state=42)\n",
    "            real_test_df = real_test_df.sample(n=min(n_samples, len(real_test_df)), random_state=42)\n",
    "            synthetic_df = synthetic_df.sample(n=min(n_samples, len(synthetic_df)), random_state=42)\n",
    "        \n",
    "        self.real_train_df = real_train_df\n",
    "        self.synthetic_df = synthetic_df\n",
    "\n",
    "        if max_seq_len is None:\n",
    "            check_col = self.temp_num_cols_[0] if len(self.temp_num_cols_) > 0 else self.temp_cat_cols_[0]\n",
    "            self.max_seq_len_ = real_train_df[check_col].apply(len).max()\n",
    "        else:\n",
    "            self.max_seq_len_ = max_seq_len\n",
    "        print(f\"Using max_seq_len: {self.max_seq_len_}\")\n",
    "\n",
    "        self.static_encoder_ = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        if self.static_cat_cols_:\n",
    "            static_fit_df = real_train_df[self.static_cat_cols_].explode(self.static_cat_cols_).fillna('missing')\n",
    "            self.static_encoder_.fit(static_fit_df)\n",
    "            print(f\"Fitted static OneHotEncoder on {len(self.static_encoder_.get_feature_names_out())} features.\")\n",
    "\n",
    "        self.temp_encoder_ = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        if self.temp_cat_cols_:\n",
    "            temp_fit_df = real_train_df[self.temp_cat_cols_].explode(self.temp_cat_cols_).fillna('missing')\n",
    "            self.temp_encoder_.fit(temp_fit_df)\n",
    "            print(f\"Fitted temporal OneHotEncoder on {len(self.temp_encoder_.get_feature_names_out())} features.\")\n",
    "            \n",
    "        print(\"Flattening and padding training data...\")\n",
    "        self.X_train_flat = self._preprocess_wide_df(real_train_df)\n",
    "        \n",
    "        print(\"Flattening and padding test data...\")\n",
    "        self.X_test_flat = self._preprocess_wide_df(real_test_df)\n",
    "        \n",
    "        print(\"Flattening and padding synthetic data...\")\n",
    "        self.X_synth_flat = self._preprocess_wide_df(synthetic_df)\n",
    "\n",
    "        if self.use_pca_:\n",
    "            print(f\"Applying PCA, reducing to {n_components_pca} components...\")\n",
    "            self.pca_ = PCA(n_components=n_components_pca)\n",
    "            self.X_train_flat = self.pca_.fit_transform(self.X_train_flat)\n",
    "            print(f\"PCA explained variance: {np.sum(self.pca_.explained_variance_ratio_):.4f}\")\n",
    "            self.X_test_flat = self.pca_.transform(self.X_test_flat)\n",
    "            self.X_synth_flat = self.pca_.transform(self.X_synth_flat)\n",
    "        \n",
    "        print(\"Scaling data...\")\n",
    "        self.scaler_ = StandardScaler()\n",
    "        self.X_train_scaled = self.scaler_.fit_transform(self.X_train_flat)\n",
    "        self.X_test_scaled = self.scaler_.transform(self.X_test_flat)\n",
    "        self.X_synth_scaled = self.scaler_.transform(self.X_synth_flat)\n",
    "        \n",
    "        print(\"Data processing complete. Evaluator is ready.\")\n",
    "\n",
    "    def _preprocess_wide_df(self, df):\n",
    "        all_patient_vectors = []\n",
    "        \n",
    "        n_static_num = len(self.static_num_cols_)\n",
    "        n_static_cat = len(self.static_encoder_.get_feature_names_out()) if self.static_cat_cols_ else 0\n",
    "        n_temp_num = len(self.temp_num_cols_)\n",
    "        n_temp_cat = len(self.temp_encoder_.get_feature_names_out()) if self.temp_cat_cols_ else 0\n",
    "        \n",
    "        df = df.sort_index()\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            \n",
    "            static_num_vector = [row[col][0] if len(row[col]) > 0 else 0 for col in self.static_num_cols_]\n",
    "            \n",
    "            static_cat_vector = []\n",
    "            if self.static_cat_cols_:\n",
    "                static_cat_list = [row[col][0] if len(row[col]) > 0 else 'missing' for col in self.static_cat_cols_]\n",
    "                static_cat_df = pd.DataFrame([static_cat_list], columns=self.static_cat_cols_)\n",
    "                static_cat_vector = self.static_encoder_.transform(static_cat_df).flatten()\n",
    "\n",
    "            static_vector = np.hstack([static_num_vector, static_cat_vector])\n",
    "            \n",
    "            temp_vector_padded = np.zeros((self.max_seq_len_, n_temp_num + n_temp_cat))\n",
    "            \n",
    "            current_max_len = 0\n",
    "            if self.temp_num_cols_:\n",
    "                 non_empty_num_cols = [col for col in self.temp_num_cols_ if len(row[col]) > 0]\n",
    "                 if non_empty_num_cols:\n",
    "                     current_max_len = max(current_max_len, max(len(row[col]) for col in non_empty_num_cols))\n",
    "            if self.temp_cat_cols_:\n",
    "                 non_empty_cat_cols = [col for col in self.temp_cat_cols_ if len(row[col]) > 0]\n",
    "                 if non_empty_cat_cols:\n",
    "                      current_max_len = max(current_max_len, max(len(row[col]) for col in non_empty_cat_cols))\n",
    "\n",
    "            if current_max_len == 0:\n",
    "                 flat_temp_vector = temp_vector_padded.flatten()\n",
    "                 final_patient_vector = np.hstack([static_vector, flat_temp_vector])\n",
    "                 all_patient_vectors.append(final_patient_vector)\n",
    "                 continue\n",
    "\n",
    "            padded_num_data_list = []\n",
    "            if self.temp_num_cols_:\n",
    "                for col in self.temp_num_cols_:\n",
    "                    original_list = row[col]\n",
    "                    padding_needed = current_max_len - len(original_list)\n",
    "                    padded_list = [np.nan] * padding_needed + list(original_list) \n",
    "                    padded_num_data_list.append(padded_list)\n",
    "                num_data_array = pd.DataFrame(padded_num_data_list).T.fillna(0).values \n",
    "            else:\n",
    "                 num_data_array = np.array([]).reshape(current_max_len, 0)\n",
    "            \n",
    "            padded_cat_data_list = []\n",
    "            if self.temp_cat_cols_:\n",
    "                for col in self.temp_cat_cols_:\n",
    "                    original_list = row[col]\n",
    "                    padding_needed = current_max_len - len(original_list)\n",
    "                    padded_list = ['missing'] * padding_needed + list(original_list)\n",
    "                    padded_cat_data_list.append(padded_list)\n",
    "                cat_data_df = pd.DataFrame(padded_cat_data_list).T.fillna('missing') \n",
    "                cat_data_encoded = self.temp_encoder_.transform(cat_data_df)\n",
    "            else:\n",
    "                 cat_data_encoded = np.array([]).reshape(current_max_len, 0)\n",
    "                 \n",
    "            seq_len = current_max_len\n",
    "\n",
    "            if seq_len > 0:\n",
    "                combined_temp_features = np.hstack([num_data_array, cat_data_encoded])\n",
    "                \n",
    "                actual_len = min(seq_len, self.max_seq_len_)\n",
    "                temp_vector_padded[-actual_len:] = combined_temp_features[-actual_len:] \n",
    "            \n",
    "            flat_temp_vector = temp_vector_padded.flatten()\n",
    "            \n",
    "            final_patient_vector = np.hstack([static_vector, flat_temp_vector])\n",
    "            all_patient_vectors.append(final_patient_vector)\n",
    "                \n",
    "        return np.vstack(all_patient_vectors)\n",
    "\n",
    "    \n",
    "    def membership_inference_attack(self, n_neighbors=1):\n",
    "        if self.X_train_scaled.size == 0 or self.X_synth_scaled.size == 0:\n",
    "            return 0.5\n",
    "\n",
    "        nn_model = KNeighborsClassifier(n_neighbors=n_neighbors, n_jobs=-1)\n",
    "        nn_model.fit(self.X_synth_scaled, np.zeros(len(self.X_synth_scaled)))\n",
    "        \n",
    "        distances_train, _ = nn_model.kneighbors(self.X_train_scaled)\n",
    "        d_train = np.mean(distances_train, axis=1)\n",
    "\n",
    "        distances_test, _ = nn_model.kneighbors(self.X_test_scaled)\n",
    "        d_test = np.mean(distances_test, axis=1)\n",
    "\n",
    "        X_attack = np.concatenate([d_train, d_test]).reshape(-1, 1)\n",
    "        y_attack = np.concatenate([np.ones(len(d_train)), np.zeros(len(d_test))])\n",
    "        \n",
    "        try:\n",
    "            X_train_mi, X_test_mi, y_train_mi, y_test_mi = train_test_split(\n",
    "                X_attack, y_attack, test_size=0.3, random_state=42, stratify=y_attack\n",
    "            )\n",
    "        except ValueError: # Handle cases with only one class in y_attack\n",
    "             X_train_mi, X_test_mi, y_train_mi, y_test_mi = train_test_split(\n",
    "                X_attack, y_attack, test_size=0.3, random_state=42\n",
    "            )\n",
    "        \n",
    "        clf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "        clf.fit(X_train_mi, y_train_mi)\n",
    "        \n",
    "        try:\n",
    "            if len(np.unique(y_test_mi)) > 1:\n",
    "                y_pred_proba = clf.predict_proba(X_test_mi)[:, 1]\n",
    "                auc = roc_auc_score(y_test_mi, y_pred_proba)\n",
    "            else: # Only one class in test set\n",
    "                auc = 0.5\n",
    "        except Exception:\n",
    "            y_pred = clf.predict(X_test_mi)\n",
    "            auc = accuracy_score(y_test_mi, y_pred)\n",
    "            \n",
    "        return auc\n",
    "\n",
    "    def re_identification_attack(self, k=1):\n",
    "        if self.X_train_scaled.size == 0 or self.X_synth_scaled.size == 0:\n",
    "            return 0.0, 0.0\n",
    "        \n",
    "        nn_model = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n",
    "        nn_model.fit(self.X_train_scaled, np.zeros(len(self.X_train_scaled)))\n",
    "        \n",
    "        synth_distances, _ = nn_model.kneighbors(self.X_synth_scaled)\n",
    "        min_synth_distances = synth_distances[:, 0]\n",
    "\n",
    "        test_distances, _ = nn_model.kneighbors(self.X_test_scaled)\n",
    "        min_test_distances = test_distances[:, 0]\n",
    "\n",
    "        non_zero_test_dist = min_test_distances[min_test_distances > 1e-6]\n",
    "        if len(non_zero_test_dist) > 0:\n",
    "            distance_threshold = np.percentile(non_zero_test_dist, 5)\n",
    "        else:\n",
    "            distance_threshold = 1e-6\n",
    "\n",
    "        attack_success_rate = np.sum(min_synth_distances < distance_threshold) / len(min_synth_distances)\n",
    "        \n",
    "        baseline_success_rate = np.sum(min_test_distances < distance_threshold) / len(min_test_distances)\n",
    "        \n",
    "        return attack_success_rate, baseline_success_rate\n",
    "    \n",
    "    def attribute_inference_attack(self, target_attribute_col):\n",
    "        if target_attribute_col not in self.static_cat_cols_:\n",
    "             print(f\"Warning: Attribute '{target_attribute_col}' is not in static_cat_cols. Skipping.\")\n",
    "             return {'real_data_auc': 0.5, 'synthetic_data_auc': 0.5}\n",
    "\n",
    "        def get_target_vector(df):\n",
    "            return df[target_attribute_col].apply(\n",
    "                lambda x: next((item for item in x if pd.notna(item) and item != 'missing'), 'missing')\n",
    "                if (isinstance(x, (list, np.ndarray)) and len(x) > 0) else 'missing'\n",
    "            ).fillna('missing').values\n",
    "\n",
    "        try:\n",
    "            y_real = get_target_vector(self.real_train_df)\n",
    "            y_synth = get_target_vector(self.synthetic_df)\n",
    "            \n",
    "            X_real = self.X_train_scaled\n",
    "            X_synth = self.X_synth_scaled\n",
    "\n",
    "        except (KeyError, IndexError):\n",
    "            print(f\"Warning: Could not extract attribute {target_attribute_col}.\")\n",
    "            return {'real_data_auc': 0.5, 'synthetic_data_auc': 0.5}\n",
    "\n",
    "        unique_values = np.unique(y_real)\n",
    "        if len(unique_values) <= 1:\n",
    "             print(f\"Warning: Attribute {target_attribute_col} has only one class.\")\n",
    "             return {'real_data_auc': 0.5, 'synthetic_data_auc': 0.5}\n",
    "        elif len(unique_values) > 2:\n",
    "            most_common = Counter(y_real).most_common(1)[0][0]\n",
    "            y_real_binary = (y_real == most_common).astype(int)\n",
    "            y_synth_binary = (y_synth == most_common).astype(int)\n",
    "        else:\n",
    "            le = LabelEncoder().fit(y_real)\n",
    "            y_real_binary = le.transform(y_real)\n",
    "            y_synth_binary = le.transform(y_synth)\n",
    "            \n",
    "        try:\n",
    "            X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(\n",
    "                X_real, y_real_binary, test_size=0.3, random_state=42, stratify=y_real_binary\n",
    "            )\n",
    "        except ValueError:\n",
    "             X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(\n",
    "                X_real, y_real_binary, test_size=0.3, random_state=42\n",
    "            )\n",
    "        \n",
    "        clf_real = KNeighborsClassifier(n_neighbors=5)\n",
    "        clf_real.fit(X_train_real, y_train_real)\n",
    "        try:\n",
    "            if len(np.unique(y_test_real)) > 1:\n",
    "                y_pred_real = clf_real.predict_proba(X_test_real)[:, 1]\n",
    "                auc_real = roc_auc_score(y_test_real, y_pred_real)\n",
    "            else:\n",
    "                auc_real = 0.5\n",
    "        except Exception:\n",
    "            auc_real = 0.5\n",
    "\n",
    "        clf_synth = KNeighborsClassifier(n_neighbors=5)\n",
    "        clf_synth.fit(X_synth, y_synth_binary)\n",
    "        try:\n",
    "            if len(np.unique(y_test_real)) > 1:\n",
    "                y_pred_synth = clf_synth.predict_proba(X_test_real)[:, 1] \n",
    "                auc_synth = roc_auc_score(y_test_real, y_pred_synth)\n",
    "            else:\n",
    "                auc_synth = 0.5\n",
    "        except Exception:\n",
    "            auc_synth = 0.5\n",
    "        \n",
    "        return {\n",
    "            'real_data_auc': auc_real,\n",
    "            'synthetic_data_auc': auc_synth\n",
    "        }\n",
    "\n",
    "    def evaluate_all_attacks(self, sensitive_attribute_cols):\n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "        print(\"Running Privacy Evaluation...\")\n",
    "        print(\"=\"*30)\n",
    "        \n",
    "        mi_auc = self.membership_inference_attack()\n",
    "        print(f\"\\n--- 1. Membership Inference Attack ---\")\n",
    "        print(f\"Attack Classifier AUC: {mi_auc:.4f}\")\n",
    "        print(f\"(Ideal value is 0.5. Higher indicates a privacy leak.)\")\n",
    "        \n",
    "        reid_attack, reid_baseline = self.re_identification_attack()\n",
    "        print(f\"\\n--- 2. Re-identification Attack ---\")\n",
    "        print(f\"Attack Success Rate: {reid_attack:.4f}\")\n",
    "        print(f\"Baseline (Ideal) Rate: {reid_baseline:.4f} (5th percentile of test set)\")\n",
    "        print(f\"(Lower is better. Attack score should be close to baseline.)\")\n",
    "\n",
    "        print(f\"\\n--- 3. Attribute Inference Attack ---\")\n",
    "        \n",
    "        results = {\n",
    "            'membership_inference_auc': mi_auc,\n",
    "            're_identification_risk': reid_attack,\n",
    "            're_identification_baseline': reid_baseline,\n",
    "            'attribute_inference': {}\n",
    "        }\n",
    "        \n",
    "        for attr_col in sensitive_attribute_cols:\n",
    "            if attr_col not in self.all_cat_cols_:\n",
    "                print(f\"Warning: Sensitive column '{attr_col}' not found. Skipping.\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                attr_result = self.attribute_inference_attack(attr_col)\n",
    "                print(f\"\\nTarget: '{attr_col}'\")\n",
    "                print(f\"  Baseline (Train on Real) AUC: {attr_result['real_data_auc']:.4f}\")\n",
    "                print(f\"  Attack (Train on Synth) AUC: {attr_result['synthetic_data_auc']:.4f}\")\n",
    "                print(f\"  (Privacy is preserved if 'Attack' AUC is not higher than 'Baseline' AUC.)\")\n",
    "                results['attribute_inference'][attr_col] = attr_result\n",
    "            except Exception as e:\n",
    "                print(f\"Could not evaluate '{attr_col}': {e}\")\n",
    "                results['attribute_inference'][attr_col] = {'real_data_auc': 0.0, 'synthetic_data_auc': 0.0}\n",
    "\n",
    "        print(\"=\"*30)\n",
    "        print(\"Privacy Evaluation Complete.\")\n",
    "        print(\"=\"*30)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9861587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "real_train_wide, real_test_wide = train_test_split(\n",
    "    final_real,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Total real patients: {len(final_real)}\")\n",
    "print(f\"Split into Real Train patients: {len(real_train_wide)}\")\n",
    "print(f\"Split into Real Test patients:  {len(real_test_wide)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0464d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = list(tn_gen.columns[1:]) + list(sn_gen.columns)\n",
    "cat_cols = list(tc_gen.columns[1:]) + list(sc_gen.columns)\n",
    "sensitive_cols = ['RACE','GENDER','ETHNICITY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4af407",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_num_cols = list(sn_gen.columns)\n",
    "static_cat_cols = list(sc_gen.columns)\n",
    "temp_num_cols = list(tn_gen.columns[1:])\n",
    "temp_cat_cols = list(tc_gen.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afabaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- RUNNING QUICK TEST WITH SAMPLING (n=5000) ---\")\n",
    "\n",
    "evaluator_sampled = PrivacyEvaluator(\n",
    "    real_train_df=real_train_wide,\n",
    "    real_test_df=real_test_wide,\n",
    "    synthetic_df=final_gen,\n",
    "    static_num_cols=static_num_cols,\n",
    "    static_cat_cols=static_cat_cols,\n",
    "    temp_num_cols=temp_num_cols,\n",
    "    temp_cat_cols=temp_cat_cols,\n",
    "    n_samples=5000 \n",
    ")\n",
    "\n",
    "results_sampled = evaluator_sampled.evaluate_all_attacks(\n",
    "    sensitive_attribute_cols=sensitive_cols\n",
    ")\n",
    "\n",
    "print(\"\\n--- SAMPLING RESULTS ---\")\n",
    "print(results_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b956c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = []\n",
    "\n",
    "# Membership Inference\n",
    "table_data.append({\n",
    "    'Privacy Attack': 'Membership Inference',\n",
    "    'Metric': 'Attack AUC',\n",
    "    'Ideal Score': '0.5000',\n",
    "    'Our Score': f\"{results_sampled['membership_inference_auc']:.4f}\",\n",
    "    'Interpretation': 'Excellent (Very close to ideal)'\n",
    "})\n",
    "\n",
    "# Re-identification\n",
    "table_data.append({\n",
    "    'Privacy Attack': 'Re-identification',\n",
    "    'Metric': 'Attack Success Rate',\n",
    "    'Ideal Score': f\"≤ Baseline ({results_sampled['re_identification_baseline']:.4f})\",\n",
    "    'Our Score': f\"{results_sampled['re_identification_risk']:.4f}\",\n",
    "    'Interpretation': 'Excellent (Score < Baseline)'\n",
    "})\n",
    "\n",
    "# Attribute Inference (one row per attribute)\n",
    "for attribute, scores in results_sampled['attribute_inference'].items():\n",
    "    interpretation = 'Good (Attack ≤ Baseline)' if scores['synthetic_data_auc'] <= scores['real_data_auc'] else 'Potential Leak (Attack > Baseline)'\n",
    "    table_data.append({\n",
    "        'Privacy Attack': f'Attribute Inference ({attribute})',\n",
    "        'Metric': 'Attack AUC',\n",
    "        'Ideal Score': f\"≤ Baseline ({scores['real_data_auc']:.4f})\",\n",
    "        'Our Score': f\"{scores['synthetic_data_auc']:.4f}\",\n",
    "        'Interpretation': interpretation\n",
    "    })\n",
    "\n",
    "privacy_summary_df = pd.DataFrame(table_data) \n",
    "\n",
    "print(\"--- Privacy Evaluation Summary Table ---\")\n",
    "print(privacy_summary_df.to_string(index=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2316c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26845c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_data = results_sampled['attribute_inference']\n",
    "attributes = list(attribute_data.keys())\n",
    "baseline_aucs = [attribute_data[attr]['real_data_auc'] for attr in attributes]\n",
    "attack_aucs = [attribute_data[attr]['synthetic_data_auc'] for attr in attributes]\n",
    "\n",
    "plot_df = pd.DataFrame({\n",
    "    'Attribute': attributes,\n",
    "    'Baseline AUC (Train on Real)': baseline_aucs,\n",
    "    'Attack AUC (Train on Synthetic)': attack_aucs\n",
    "})\n",
    "\n",
    "x = np.arange(len(attributes))  \n",
    "width = 0.35 \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "rects1 = ax.bar(x - width/2, plot_df['Baseline AUC (Train on Real)'], width, \n",
    "                label='Baseline (Train on Real)', color='skyblue')\n",
    "rects2 = ax.bar(x + width/2, plot_df['Attack AUC (Train on Synthetic)'], width, \n",
    "                label='Attack (Train on Synthetic)', color='lightcoral')\n",
    "\n",
    "ax.set_ylabel('AUC Score')\n",
    "ax.set_title('Attribute Inference Attack Results (AUC)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(attributes)\n",
    "ax.legend(loc='lower right') \n",
    "ax.axhline(0.5, color='grey', linestyle='--', linewidth=0.8, label='Random Guess (AUC=0.5)')\n",
    "ax.legend()\n",
    "min_auc = min(min(baseline_aucs), min(attack_aucs))\n",
    "max_auc = max(max(baseline_aucs), max(attack_aucs))\n",
    "ax.set_ylim([max(0, min_auc - 0.05), min(1, max_auc + 0.05)]) \n",
    "ax.bar_label(rects1, padding=3, fmt='%.3f', fontsize=9)\n",
    "ax.bar_label(rects2, padding=3, fmt='%.3f', fontsize=9)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.xticks(rotation=0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623fada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
