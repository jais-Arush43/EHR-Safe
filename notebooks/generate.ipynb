{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae71ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:51:02.461006Z",
     "iopub.status.busy": "2025-10-17T15:51:02.460542Z",
     "iopub.status.idle": "2025-10-17T15:51:06.765750Z",
     "shell.execute_reply": "2025-10-17T15:51:06.763823Z",
     "shell.execute_reply.started": "2025-10-17T15:51:02.460965Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader,random_split,TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import json\n",
    "import joblib\n",
    "import ast\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272a9aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:51:06.769851Z",
     "iopub.status.busy": "2025-10-17T15:51:06.769185Z",
     "iopub.status.idle": "2025-10-17T15:51:06.776386Z",
     "shell.execute_reply": "2025-10-17T15:51:06.775054Z",
     "shell.execute_reply.started": "2025-10-17T15:51:06.769818Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c3a67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:51:06.778221Z",
     "iopub.status.busy": "2025-10-17T15:51:06.777800Z",
     "iopub.status.idle": "2025-10-17T15:51:06.803723Z",
     "shell.execute_reply": "2025-10-17T15:51:06.801826Z",
     "shell.execute_reply.started": "2025-10-17T15:51:06.778186Z"
    }
   },
   "outputs": [],
   "source": [
    "class StaticOneHotManager:\n",
    "    def __init__(self):\n",
    "        self.encoders = {}\n",
    "\n",
    "    def fit(self, df, categorical_cols):\n",
    "        for col in categorical_cols:\n",
    "            categories = df[col].dropna().astype(str).unique().tolist()\n",
    "\n",
    "            enc = OneHotEncoder(\n",
    "                sparse_output=False,\n",
    "                handle_unknown=\"ignore\",\n",
    "                categories=[categories]\n",
    "            )\n",
    "            values = df[col].dropna().astype(str).values.reshape(-1, 1)\n",
    "            enc.fit(values)\n",
    "\n",
    "            self.encoders[col] = enc\n",
    "\n",
    "    def transform(self, df, categorical_cols, return_numpy=True):\n",
    "        out = pd.DataFrame()\n",
    "        if \"PATIENT\" in df.columns:\n",
    "            out[\"PATIENT\"] = df[\"PATIENT\"]\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            enc = self.encoders[col]\n",
    "\n",
    "            values = df[col].astype(str).values.reshape(-1, 1)\n",
    "            is_na = df[col].isna().values\n",
    "\n",
    "            encoded = enc.transform(values)\n",
    "\n",
    "            encoded[is_na] = 0\n",
    "            if return_numpy:\n",
    "                out[col] = [encoded[i, :] for i in range(encoded.shape[0])]\n",
    "            else:\n",
    "                out[col] = encoded.tolist()\n",
    "\n",
    "        return out\n",
    "\n",
    "    def inverse_transform(self, series, col):\n",
    "        enc = self.encoders[col]\n",
    "        categories = enc.categories_[0]  \n",
    "        results = []\n",
    "    \n",
    "        for arr in series:\n",
    "            arr = np.array(arr, dtype=float)  \n",
    "            if arr.sum() == 0: \n",
    "                results.append(np.nan)\n",
    "            else:\n",
    "                idx = arr.argmax()  \n",
    "                results.append(categories[idx])\n",
    "    \n",
    "        return results\n",
    "\n",
    "    def save(self, path=\"static_encoded.pkl\"):\n",
    "        joblib.dump(self.encoders, path)\n",
    "\n",
    "    def load(self, path=\"static_encoded.pkl\"):\n",
    "        self.encoders = joblib.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f82a86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:42:49.250148Z",
     "iopub.status.busy": "2025-10-17T16:42:49.249766Z",
     "iopub.status.idle": "2025-10-17T16:42:49.268298Z",
     "shell.execute_reply": "2025-10-17T16:42:49.266828Z",
     "shell.execute_reply.started": "2025-10-17T16:42:49.250121Z"
    }
   },
   "outputs": [],
   "source": [
    "class TemporalOneHotManager:\n",
    "    def __init__(self):\n",
    "        self.encoders = {}\n",
    "\n",
    "    def fit(self, df, categorical_cols):\n",
    "        for col in categorical_cols:\n",
    "            flat_values = []\n",
    "            for seq in df[col]:\n",
    "                if pd.isna(seq):\n",
    "                    continue\n",
    "                if isinstance(seq, str):\n",
    "                    try:\n",
    "                        seq = ast.literal_eval(seq.replace('nan', 'None'))\n",
    "                    except (ValueError, SyntaxError):\n",
    "                        seq = [seq]\n",
    "                if not isinstance(seq, list):\n",
    "                    seq = [seq]\n",
    "                for v in seq:\n",
    "                    if pd.notna(v):\n",
    "                        flat_values.append(str(v))\n",
    "\n",
    "            categories = pd.Series(flat_values).unique().tolist()\n",
    "            enc = OneHotEncoder(\n",
    "                sparse_output=False,\n",
    "                handle_unknown=\"ignore\",\n",
    "                categories=[categories]\n",
    "            )\n",
    "            enc.fit(np.array(categories).reshape(-1, 1))\n",
    "            self.encoders[col] = enc\n",
    "\n",
    "    def transform(self, df, categorical_cols, return_numpy=True):\n",
    "        out = pd.DataFrame()\n",
    "        if \"PATIENT\" in df.columns:\n",
    "            out[\"PATIENT\"] = df[\"PATIENT\"]\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            enc = self.encoders[col]\n",
    "            cats = enc.categories_[0].tolist()\n",
    "            n_cats = len(cats)\n",
    "\n",
    "            rows = []\n",
    "            for seq in df[col]:\n",
    "                if pd.isna(seq):\n",
    "                    seq = []\n",
    "                elif isinstance(seq, str):\n",
    "                    try:\n",
    "                        seq = ast.literal_eval(seq.replace('nan', 'None'))\n",
    "                    except (ValueError, SyntaxError):\n",
    "                        seq = [seq]\n",
    "\n",
    "                if not isinstance(seq, list):\n",
    "                    seq = [seq]\n",
    "\n",
    "                encoded_seq = []\n",
    "                for v in seq:\n",
    "                    if pd.isna(v):\n",
    "                        if n_cats > 0:\n",
    "                            encoded_seq.append([0] * n_cats)\n",
    "                    else:\n",
    "                        if n_cats > 0:\n",
    "                            arr = enc.transform([[str(v)]]).flatten()\n",
    "                            encoded_seq.append(arr.astype(int).tolist())\n",
    "                rows.append(encoded_seq)\n",
    "            out[col] = rows\n",
    "\n",
    "        return out\n",
    "\n",
    "    def inverse_transform(self, series, col):\n",
    "        enc = self.encoders[col]\n",
    "        cats = enc.categories_[0].tolist()\n",
    "        results = []\n",
    "    \n",
    "        for row in series:\n",
    "            arr = np.array(row, dtype=float)\n",
    "            if arr.sum() == 0:\n",
    "                results.append(np.nan)\n",
    "            else:\n",
    "                results.append(cats[arr.argmax()])\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def inverse_transform2(self, series, col):\n",
    "        enc = self.encoders[col]\n",
    "        categories = enc.categories_[0]\n",
    "        results = []\n",
    "        \n",
    "        for idx in series:\n",
    "            if pd.isna(idx) or idx == 0:\n",
    "                results.append(np.nan)\n",
    "            else:\n",
    "                results.append(categories[int(idx)])\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save(self, path=\"temporal_encoded.pkl\"):\n",
    "        joblib.dump(self.encoders, path)\n",
    "\n",
    "    def load(self, path=\"temporal_encoded.pkl\"):\n",
    "        self.encoders = joblib.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d78bbc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:51:06.846813Z",
     "iopub.status.busy": "2025-10-17T15:51:06.846525Z",
     "iopub.status.idle": "2025-10-17T15:51:06.883980Z",
     "shell.execute_reply": "2025-10-17T15:51:06.882663Z",
     "shell.execute_reply.started": "2025-10-17T15:51:06.846792Z"
    }
   },
   "outputs": [],
   "source": [
    "class CategoricalAutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dims, lr=1e-3, optimizer_type=\"adam\", use_scheduler=False, filename=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dims = input_dims\n",
    "        self.filename = filename\n",
    "        self.total_input_dim = sum(self.input_dims)\n",
    "        self.num_features = len(self.input_dims)\n",
    "        hidden_dim = max(256, self.total_input_dim)\n",
    "        latent_dim = max(32, min(self.total_input_dim // 8, self.total_input_dim - 1))\n",
    "\n",
    "        # ------ Encoder -----------\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.total_input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_dim // 2, latent_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # -------- Decoders: one head per feature ----------\n",
    "        self.decoders = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim // 2),\n",
    "                nn.BatchNorm1d(hidden_dim // 2),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                \n",
    "                nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(0.3),\n",
    "                \n",
    "                nn.Linear(hidden_dim, k)\n",
    "            )\n",
    "            for k in self.input_dims\n",
    "        ])\n",
    "        \n",
    "        #--------- Loss & Optimizer ---------\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        if optimizer_type == \"adam\":\n",
    "            self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        elif optimizer_type.lower() == \"rmsprop\":\n",
    "            self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
    "        else:\n",
    "            raise ValueError(\"optimizer_type must be Adam or RMSprop\")\n",
    "\n",
    "        self.scheduler = (optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min',\n",
    "                                                              factor=0.5, patience=3)\n",
    "                         if use_scheduler else None)\n",
    "\n",
    "    # ---------- Forward / Encode / Decode ----------\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        logits_list = [head(z) for head in self.decoders]\n",
    "        return logits_list\n",
    "\n",
    "    def encode(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            z = self.encoder(x)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z: torch.Tensor, mask=None, return_onehot=True):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_list = [head(z) for head in self.decoders]\n",
    "            preds = [logits.argmax(dim=-1) for logits in logits_list]\n",
    "    \n",
    "            if not return_onehot:\n",
    "                return preds\n",
    "    \n",
    "            bsz = z.shape[0]\n",
    "            parts = []\n",
    "    \n",
    "            for i, (idxs, K) in enumerate(zip(preds, self.input_dims)):\n",
    "                onehot = torch.zeros(bsz, K, device=z.device)\n",
    "                onehot.scatter_(1, idxs.view(-1, 1), 1.0)\n",
    "                \n",
    "                if mask is not None:\n",
    "                    # mask[:, i] has shape (batch_size,)\n",
    "                    onehot = onehot * mask[:, i].unsqueeze(1)  # broadcast to (batch_size, K)\n",
    "                \n",
    "                parts.append(onehot)\n",
    "    \n",
    "            return preds, torch.cat(parts, dim=1)\n",
    "\n",
    "\n",
    "    def predict_prob(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_list = self.forward(x)\n",
    "            probs_list = [F.softmax(logits, dim=-1) for logits in logits_list]\n",
    "        return probs_list\n",
    "\n",
    "    # ---------- Reconstruct with mask for missingness ----------\n",
    "    def reconstruct(self, x, return_onehot=False):\n",
    "        probs_list = self.predict_prob(x)\n",
    "        preds = []\n",
    "        bsz = x.shape[0]\n",
    "        parts = []\n",
    "\n",
    "        start = 0\n",
    "        for i, K in enumerate(self.input_dims):\n",
    "            end = start + K\n",
    "            segment = x[:, start:end]\n",
    "\n",
    "            # All-zero mask for missing features\n",
    "            mask_all_zero = (segment.sum(dim=1) == 0)\n",
    "            segment_logits = probs_list[i]\n",
    "            segment_pred = segment_logits.argmax(dim=-1)\n",
    "            preds.append(segment_pred)\n",
    "\n",
    "            if return_onehot:\n",
    "                onehot = torch.zeros(bsz, K, device=x.device)\n",
    "                idxs_to_scatter = (~mask_all_zero).nonzero(as_tuple=True)[0]\n",
    "                if len(idxs_to_scatter) > 0:\n",
    "                    onehot[idxs_to_scatter].scatter_(1, segment_pred[idxs_to_scatter].view(-1,1), 1.0)\n",
    "                parts.append(onehot)\n",
    "\n",
    "            start = end\n",
    "\n",
    "        if return_onehot:\n",
    "            return preds, torch.cat(parts, dim=1)\n",
    "        return preds\n",
    "\n",
    "    # ---------- Helper functions ----------\n",
    "    def _targets_from_onehot(self, x):\n",
    "        parts = torch.split(x, self.input_dims, dim=1)\n",
    "        return torch.stack([p.argmax(dim=1) for p in parts], dim=1).long()\n",
    "\n",
    "    def _compute_loss(self, logits_list, targets, mask=None):\n",
    "        loss = 0.0\n",
    "        for i, logits in enumerate(logits_list):\n",
    "            if mask is not None:\n",
    "                present_idx = (mask[:, i] == 1).nonzero(as_tuple=True)[0]\n",
    "                if len(present_idx) == 0:\n",
    "                    continue  # skip missing features\n",
    "                loss += self.criterion(logits[present_idx], targets[present_idx, i])\n",
    "            else:\n",
    "                loss += self.criterion(logits, targets[:, i])\n",
    "        return loss\n",
    "\n",
    "    # ---------- Training ----------\n",
    "    def fit(self, dataloader, epochs=10, val_dataloader=None, wrapper_model=None, mask_val=None):\n",
    "        forward_model = wrapper_model if wrapper_model is not None else self\n",
    "        best_val_loss = float('inf')\n",
    "        best_weights = None\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.train()\n",
    "            epoch_losses = []\n",
    "\n",
    "            for (x,) in tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\", leave=False):\n",
    "                x = x.to(next(self.parameters()).device, non_blocking=True)\n",
    "                y = self._targets_from_onehot(x)\n",
    "                self.optimizer.zero_grad()\n",
    "                logits_list = forward_model(x)\n",
    "                loss = self._compute_loss(logits_list, y)\n",
    "                if wrapper_model is not None:\n",
    "                    loss = loss.mean()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_losses.append(loss.item())\n",
    "\n",
    "            train_loss = sum(epoch_losses) / max(1, len(epoch_losses))\n",
    "\n",
    "            val_loss = None\n",
    "            if val_dataloader is not None:\n",
    "                self.eval()\n",
    "                val_losses = []\n",
    "                with torch.no_grad():\n",
    "                    for i, (vx,) in enumerate(val_dataloader):\n",
    "                        vx = vx.to(next(self.parameters()).device, non_blocking=True)\n",
    "                        vy = self._targets_from_onehot(vx)\n",
    "                        vlogits_list = forward_model(vx)\n",
    "                        if mask_val is not None:\n",
    "                            vloss = self._compute_loss(vlogits_list, vy, mask=mask_val[i*vx.size(0):(i+1)*vx.size(0)])\n",
    "                        else:\n",
    "                            vloss = self._compute_loss(vlogits_list, vy)\n",
    "                        val_losses.append(vloss.item())\n",
    "                val_loss = sum(val_losses) / max(1, len(val_losses))\n",
    "\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step(val_loss)\n",
    "\n",
    "                # Save best weights\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_weights = self.state_dict()\n",
    "\n",
    "            print(f\"Epoch {epoch:03d} | Train {train_loss:.7f} | \"\n",
    "                  f\"Val {val_loss:.7f}\" if val_loss is not None else f\"Epoch {epoch:03d} | Train {train_loss:.7f}\")\n",
    "\n",
    "        # Save best weights\n",
    "        if best_weights is not None:\n",
    "            torch.save(best_weights, self.filename)\n",
    "            print(f\"Best model saved with val_loss={best_val_loss:.7f} as {self.filename}\")\n",
    "        else:\n",
    "            self.save_model()\n",
    "\n",
    "    # ---------- Save / Load ----------\n",
    "    def save_model(self):\n",
    "        torch.save(self.state_dict(), self.filename)\n",
    "        print(f\"Model saved as {self.filename}\")\n",
    "\n",
    "    def load_model(self, map_location=None):\n",
    "        state = torch.load(self.filename, map_location=map_location)\n",
    "        self.load_state_dict(state)\n",
    "        self.eval()\n",
    "        print(f\"Model loaded from {self.filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0588c1e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:51:06.885650Z",
     "iopub.status.busy": "2025-10-17T15:51:06.885271Z",
     "iopub.status.idle": "2025-10-17T15:51:06.915454Z",
     "shell.execute_reply": "2025-10-17T15:51:06.913721Z",
     "shell.execute_reply.started": "2025-10-17T15:51:06.885615Z"
    }
   },
   "outputs": [],
   "source": [
    "class StochasticNormalizer:\n",
    "    def __init__(self):\n",
    "        self.params = {}\n",
    "\n",
    "    def stochastic_normalize(self, X: torch.Tensor, key=None):\n",
    "        X = X.float()\n",
    "        unique_vals, counts = torch.unique(X, return_counts=True)\n",
    "        N = X.numel()\n",
    "        X_hat = torch.empty_like(X, dtype=torch.float32, device=X.device)\n",
    "        lower_bound = 0.0\n",
    "        params = {}\n",
    "\n",
    "        for val, count in zip(unique_vals, counts):\n",
    "            ratio = count.item() / N\n",
    "            upper_bound = lower_bound + ratio\n",
    "            mask = X == val\n",
    "            X_hat[mask] = torch.rand(mask.sum(), device=X.device) * (upper_bound - lower_bound) + lower_bound\n",
    "            params[val.item()] = [lower_bound, upper_bound]\n",
    "            lower_bound = upper_bound\n",
    "\n",
    "        if key is not None:\n",
    "            self.params[key] = params\n",
    "        return X_hat\n",
    "\n",
    "    def stochastic_renormalize(self, X_hat: torch.Tensor, key=None):\n",
    "        X_hat = X_hat.float()\n",
    "        X = torch.zeros_like(X_hat, dtype=torch.float32, device=X_hat.device)\n",
    "        if key is not None:\n",
    "            params = self.params[key]\n",
    "\n",
    "        for val, (low, high) in params.items():\n",
    "            mask = (X_hat >= low) & (X_hat < high)\n",
    "            X[mask] = val\n",
    "\n",
    "        # Handle edge case where X_hat == 1.0\n",
    "        mask = X_hat == 1.0\n",
    "        for val, (low, high) in params.items():\n",
    "            if abs(high - 1.0) < 1e-8:\n",
    "                X[mask] = val\n",
    "        return X\n",
    "\n",
    "    def normalize_sample(self, X: torch.Tensor, key):\n",
    "        if key not in self.params:\n",
    "            raise ValueError(f\"No parameters found for key '{key}'. Load or train first.\")\n",
    "\n",
    "        params = self.params[key]\n",
    "        X_hat = torch.zeros_like(X, dtype=torch.float32, device=X.device)\n",
    "        for val, (low, high) in params.items():\n",
    "            mask = X == val\n",
    "            if mask.any():\n",
    "                X_hat[mask] = torch.rand(mask.sum(), device=X.device) * (high - low) + low\n",
    "        return X_hat\n",
    "\n",
    "    def save_params(self, filepath):\n",
    "        torch.save(self.params, filepath)\n",
    "\n",
    "    def load_params(self, filepath):\n",
    "        self.params = torch.load(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cefb60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:51:06.916802Z",
     "iopub.status.busy": "2025-10-17T15:51:06.916438Z",
     "iopub.status.idle": "2025-10-17T15:51:06.936631Z",
     "shell.execute_reply": "2025-10-17T15:51:06.935475Z",
     "shell.execute_reply.started": "2025-10-17T15:51:06.916769Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims=(512,256,256), out_dim=16, dropout=0.1,\n",
    "                 use_batch_norm=True, activation='lrelu', final_activation=None):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "\n",
    "        activation_fn = {\n",
    "            'lrelu': nn.LeakyReLU(0.01),\n",
    "            'relu': nn.ReLU(),\n",
    "            'gelu': nn.GELU(),\n",
    "            'swish': nn.SiLU(),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sigmoid': nn.Sigmoid()\n",
    "        }[activation]\n",
    "\n",
    "        d = in_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(d,h))\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(activation_fn)\n",
    "            if dropout > 0.0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            d = h\n",
    "\n",
    "        layers.append(nn.Linear(d, out_dim))\n",
    "        if final_activation is not None:\n",
    "            layers.append(final_activation)   \n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8cda03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:51:06.938458Z",
     "iopub.status.busy": "2025-10-17T15:51:06.938074Z",
     "iopub.status.idle": "2025-10-17T15:51:06.958390Z",
     "shell.execute_reply": "2025-10-17T15:51:06.956610Z",
     "shell.execute_reply.started": "2025-10-17T15:51:06.938430Z"
    }
   },
   "outputs": [],
   "source": [
    "class TemporalEncoder(nn.Module):\n",
    "    def __init__(self,in_dim,hidden_dim=256,out_dim=128,depth=2,use_post_mlp=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size = in_dim,\n",
    "            hidden_size = hidden_dim,\n",
    "            num_layers = depth,\n",
    "            batch_first = True,\n",
    "            dropout = 0.1\n",
    "        )\n",
    "\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        if use_post_mlp:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_dim,hidden_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim,out_dim)\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim,out_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # x: (batch, seq_len, in_dim)\n",
    "        out, _ = self.gru(x)\n",
    "        attn_scores = self.attn(out).squeeze(-1) # (batch,seq_len)\n",
    "        attn_weights = torch.softmax(attn_scores,dim=1)\n",
    "        context = torch.sum(out * attn_weights.unsqueeze(-1),dim=1) # (batch, hidden_dim)\n",
    "        return self.fc(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2e4f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:51:06.959973Z",
     "iopub.status.busy": "2025-10-17T15:51:06.959606Z",
     "iopub.status.idle": "2025-10-17T15:51:06.981723Z",
     "shell.execute_reply": "2025-10-17T15:51:06.980213Z",
     "shell.execute_reply.started": "2025-10-17T15:51:06.959913Z"
    }
   },
   "outputs": [],
   "source": [
    "class FusionMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64], output_dim=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dims = [input_dim] + hidden_dims\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            layers.append(nn.BatchNorm1d(dims[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.Linear(dims[-1], output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdf626c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:51:06.983740Z",
     "iopub.status.busy": "2025-10-17T15:51:06.983367Z",
     "iopub.status.idle": "2025-10-17T15:51:07.015104Z",
     "shell.execute_reply": "2025-10-17T15:51:07.013851Z",
     "shell.execute_reply.started": "2025-10-17T15:51:06.983702Z"
    }
   },
   "outputs": [],
   "source": [
    "class TemporalDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int, hidden_dim = 512, depth = 2,\n",
    "                 num_features=None, embed_dim=None, is_categorical=False):\n",
    "        super().__init__()\n",
    "        self.is_categorical = is_categorical\n",
    "        self.fc_init_h = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, depth * hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=latent_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=depth,\n",
    "            batch_first=True,\n",
    "            dropout=0.1 if depth > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        self.head_value = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, embed_dim if is_categorical else num_features),\n",
    "            nn.Tanh() if is_categorical else nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.head_time = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim // 8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.head_mask = nn.Sequential(\n",
    "            nn.Linear(hidden_dim  , num_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, e, max_seq_len, mask_threshold = 0.5):\n",
    "        batch_size = e.size(0)\n",
    "        device = e.device\n",
    "        h_0_flat = self.fc_init_h(e)\n",
    "        h_0 = h_0_flat.view(self.gru.num_layers, batch_size, self.gru.hidden_size)\n",
    "\n",
    "        tn_hat_list, u_hat_list, mask_hat_list = [], [], []\n",
    "        h_t = h_0\n",
    "\n",
    "        seq_lengths = torch.zeros(batch_size, dtype=torch.long, device=device)\n",
    "        active_sequences = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
    "\n",
    "        for _ in range(max_seq_len):\n",
    "            if not active_sequences.any():\n",
    "                break\n",
    "\n",
    "            gru_input = e.unsqueeze(1)\n",
    "            gru_out, h_t_new = self.gru(gru_input, h_t)\n",
    "\n",
    "            h_t = torch.where(active_sequences.view(1, -1, 1), h_t_new, h_t) \n",
    "\n",
    "            tn_hat_step = self.head_value(gru_out.squeeze(1))\n",
    "            u_hat_step = self.head_time(gru_out.squeeze(1)) \n",
    "            mask_hat_step = self.head_mask(gru_out.squeeze(1))  \n",
    "\n",
    "            tn_hat_list.append(tn_hat_step)\n",
    "            u_hat_list.append(u_hat_step)\n",
    "            mask_hat_list.append(mask_hat_step)\n",
    "\n",
    "            seq_lengths += active_sequences.long()\n",
    "\n",
    "            stop_condition = (torch.sigmoid(mask_hat_step) < mask_threshold).all(dim=-1)\n",
    "            active_sequences = active_sequences & ~stop_condition\n",
    "\n",
    "        tn_hat = torch.stack(tn_hat_list, dim=1)\n",
    "        u_hat = torch.stack(u_hat_list, dim=1)\n",
    "        mask_hat = torch.stack(mask_hat_list, dim=1)\n",
    "\n",
    "        return tn_hat, u_hat, mask_hat, seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695560e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:51:07.017237Z",
     "iopub.status.busy": "2025-10-17T15:51:07.016651Z",
     "iopub.status.idle": "2025-10-17T15:51:07.072878Z",
     "shell.execute_reply": "2025-10-17T15:51:07.071086Z",
     "shell.execute_reply.started": "2025-10-17T15:51:07.017199Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,sn_dim,sce_latent_dim,tn_dim,tce_latent_dim,sc_dim,tc_dim,latent_dim=512):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        static_input_dim = sn_dim + sce_latent_dim\n",
    "        self.static_encoder = MLP(static_input_dim, out_dim=128)\n",
    "        self.temporal_num_encoder = TemporalEncoder(tn_dim + 1, out_dim=256)\n",
    "        self.temporal_cat_encoder = TemporalEncoder(tce_latent_dim + 1, out_dim=256)\n",
    "        self.static_mask = MLP(in_dim = sn_dim + sc_dim,hidden_dims=(32,16,8),out_dim=4)\n",
    "        self.temporal_num_mask_encoder = TemporalEncoder(tn_dim,hidden_dim=128,out_dim=64)\n",
    "        self.temporal_cat_mask_encoder = TemporalEncoder(tc_dim,hidden_dim=128,out_dim=64)\n",
    "        fusion_dim = 128 + 256 + 256 + 4 + 64 + 64\n",
    "        self.fusion = FusionMLP(fusion_dim, hidden_dims=[2048, 1024], output_dim=latent_dim)\n",
    "        self.static_decoder_num = MLP(latent_dim, out_dim=sn_dim,final_activation=nn.Sigmoid())\n",
    "        self.static_decoder_cat = MLP(latent_dim,out_dim=sce_latent_dim,final_activation=nn.Tanh())\n",
    "        self.static_decoder_mask = MLP(latent_dim, out_dim=sn_dim + sc_dim)\n",
    "        self.temporal_decoder_num = TemporalDecoder(latent_dim, num_features=tn_dim, is_categorical=False)\n",
    "        self.temporal_decoder_cat = TemporalDecoder(latent_dim, num_features=tc_dim, embed_dim=tce_latent_dim,is_categorical=True)\n",
    "\n",
    "    def encode(self,sn,sc,tn,tc,un,uc,sn_mask,sc_mask,tn_mask,tc_mask):\n",
    "        static_in = torch.cat([sn,sc],dim=-1)\n",
    "        static_e = self.static_encoder(static_in)\n",
    "        un = un.unsqueeze(-1)\n",
    "        temporal_num_in = torch.cat([tn,un],dim=-1)\n",
    "        temporal_num_e = self.temporal_num_encoder(temporal_num_in)\n",
    "        uc = uc.unsqueeze(-1)\n",
    "        temporal_cat_in = torch.cat([tc,uc],dim=-1)\n",
    "        temporal_cat_e = self.temporal_cat_encoder(temporal_cat_in)\n",
    "        static_mask_in = torch.cat([sn_mask,sc_mask],dim=-1)\n",
    "        static_mask_e = self.static_mask(static_mask_in)\n",
    "        temporal_num_mask_e = self.temporal_num_mask_encoder(tn_mask)\n",
    "        temporal_cat_mask_e = self.temporal_cat_mask_encoder(tc_mask)\n",
    "        e = torch.cat([static_e,temporal_num_e,temporal_cat_e,static_mask_e,temporal_num_mask_e,temporal_cat_mask_e],dim=-1)\n",
    "        e = self.fusion(e)\n",
    "        return e\n",
    "\n",
    "\n",
    "    def decode(self, e, max_seq_len_num=100,max_seq_len_cat=300):\n",
    "        sn_hat = self.static_decoder_num(e)\n",
    "        sc_hat = self.static_decoder_cat(e)\n",
    "        static_mask_hat = self.static_decoder_mask(e)\n",
    "        sn_dim = sn_hat.shape[-1]\n",
    "        sn_mask_hat = static_mask_hat[..., :sn_dim]\n",
    "        sc_mask_hat = static_mask_hat[..., sn_dim:]\n",
    "        tn_hat, un_hat, tn_mask_hat, seq_len_num = self.temporal_decoder_num(e, max_seq_len_num)\n",
    "        tc_hat, uc_hat, tc_mask_hat, seq_len_cat = self.temporal_decoder_cat(e, max_seq_len_cat)\n",
    "        return sn_hat,sc_hat,tn_hat,tc_hat,un_hat,uc_hat,sn_mask_hat,sc_mask_hat,tn_mask_hat,tc_mask_hat,seq_len_num,seq_len_cat\n",
    "\n",
    "    def forward(self,sn,sc,tn,tc,un,uc,sn_mask,sc_mask,tn_mask,tc_mask,max_seq_len_num=None,max_seq_len_cat=None):\n",
    "        e = self.encode(sn,sc,tn,tc,un,uc,sn_mask,sc_mask,tn_mask,tc_mask)\n",
    "        if max_seq_len_num is None:\n",
    "            max_seq_len_num = tn.size(1)\n",
    "        if max_seq_len_cat is None:\n",
    "            max_seq_len_cat = tc.size(1)\n",
    "        return self.decode(e, max_seq_len_num,max_seq_len_cat)\n",
    "\n",
    "    def get_encoding(self, sn, sc, tn, tc, un, uc, sn_mask, sc_mask, tn_mask, tc_mask, as_numpy: bool = False):\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            inputs = [sn, sc, tn, tc, un, uc, sn_mask, sc_mask, tn_mask, tc_mask]\n",
    "            inputs = [x.to(device) if torch.is_tensor(x) else x for x in inputs]\n",
    "            sn, sc, tn, tc, un, uc, sn_mask, sc_mask, tn_mask, tc_mask = inputs\n",
    "            e = self.encode(sn, sc, tn, tc, un, uc, sn_mask, sc_mask, tn_mask, tc_mask)\n",
    "            if as_numpy:\n",
    "                return e.detach().cpu().numpy()\n",
    "            return e\n",
    "    \n",
    "    def generate_decoding(self, e=None, batch_size=1, max_seq_len_num=100, max_seq_len_cat=300, device=\"cpu\", mask_threshold=0.5):\n",
    "        self.eval()\n",
    "        device = torch.device(device)\n",
    "        with torch.no_grad():\n",
    "            (sn_hat, sc_hat, tn_hat, tc_hat, un_hat, uc_hat, sn_mask_hat, sc_mask_hat, tn_mask_hat, tc_mask_hat, seq_len_num, seq_len_cat) = self.decode(e, max_seq_len_num, max_seq_len_cat)\n",
    "            \n",
    "            # Apply sigmoid + threshold for all masks\n",
    "            sn_mask_hat = torch.sigmoid(sn_mask_hat) > mask_threshold\n",
    "            sc_mask_hat = torch.sigmoid(sc_mask_hat) > mask_threshold\n",
    "            tn_mask_hat = torch.sigmoid(tn_mask_hat) > mask_threshold\n",
    "            tc_mask_hat = torch.sigmoid(tc_mask_hat) > mask_threshold\n",
    "    \n",
    "            # Slice sequences according to predicted lengths\n",
    "            tn_hat = [tn_hat[i, :seq_len_num[i]] for i in range(tn_hat.size(0))]\n",
    "            tc_hat = [tc_hat[i, :seq_len_cat[i]] for i in range(tc_hat.size(0))]\n",
    "            un_hat = [un_hat[i, :seq_len_num[i]] for i in range(un_hat.size(0))]\n",
    "            uc_hat = [uc_hat[i, :seq_len_cat[i]] for i in range(uc_hat.size(0))]\n",
    "            tn_mask_hat = [tn_mask_hat[i, :seq_len_num[i]] for i in range(tn_mask_hat.size(0))]\n",
    "            tc_mask_hat = [tc_mask_hat[i, :seq_len_cat[i]] for i in range(tc_mask_hat.size(0))]\n",
    "    \n",
    "        return (sn_hat, sc_hat, sn_mask_hat, sc_mask_hat, tn_hat, tc_hat, un_hat, uc_hat, tn_mask_hat, tc_mask_hat)\n",
    "\n",
    "    def compute_loss(\n",
    "        self, sn, sc, tn, tc, un, uc, sn_mask, sc_mask, tn_mask, tc_mask,\n",
    "        sn_hat, sc_hat, sn_mask_hat, sc_mask_hat, tn_hat, tc_hat, un_hat, uc_hat,\n",
    "        tn_mask_hat, tc_mask_hat, pred_seq_len_num, pred_seq_len_cat, true_seq_len_num, true_seq_len_cat,\n",
    "        lambda_mse=1.0, lambda_len=0.1\n",
    "    ):\n",
    "        losses = {}\n",
    "        bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        mse = nn.MSELoss(reduction='none')\n",
    "        epsilon = 1e-8\n",
    "        device = sn.device\n",
    "        if not torch.is_tensor(pred_seq_len_num):\n",
    "            pred_seq_len_num = torch.tensor(pred_seq_len_num, device=device, dtype=torch.float32)\n",
    "        if pred_seq_len_num.dim() == 0:\n",
    "            pred_seq_len_num = pred_seq_len_num.unsqueeze(0)\n",
    "        if not torch.is_tensor(pred_seq_len_cat):\n",
    "            pred_seq_len_cat = torch.tensor(pred_seq_len_cat, device=device, dtype=torch.float32)\n",
    "        if pred_seq_len_cat.dim() == 0:\n",
    "            pred_seq_len_cat = pred_seq_len_cat.unsqueeze(0)\n",
    "        if not torch.is_tensor(true_seq_len_num):\n",
    "            true_seq_len_num = torch.tensor(true_seq_len_num, device=device, dtype=torch.float32)\n",
    "        if true_seq_len_num.dim() == 0:\n",
    "            true_seq_len_num = true_seq_len_num.unsqueeze(0)\n",
    "        if not torch.is_tensor(true_seq_len_cat):\n",
    "            true_seq_len_cat = torch.tensor(true_seq_len_cat, device=device, dtype=torch.float32)\n",
    "        if true_seq_len_cat.dim() == 0:\n",
    "            true_seq_len_cat = true_seq_len_cat.unsqueeze(0)\n",
    "        pred_len_num = tn_hat.size(1)\n",
    "        pred_len_cat = tc_hat.size(1)\n",
    "        tn_sliced = tn[:, :pred_len_num, :]\n",
    "        un_sliced = un[:, :pred_len_num].unsqueeze(-1)\n",
    "        tn_mask_sliced = tn_mask[:, :pred_len_num, :]\n",
    "        tc_sliced = tc[:, :pred_len_cat, :]\n",
    "        uc_sliced = uc[:, :pred_len_cat].unsqueeze(-1)\n",
    "        tc_mask_sliced = tc_mask[:, :pred_len_cat, :]\n",
    "        seq_mask_num = torch.arange(pred_len_num, device=device).unsqueeze(0) < pred_seq_len_num.unsqueeze(1)\n",
    "        seq_mask_num = seq_mask_num.unsqueeze(-1).float()\n",
    "        seq_mask_cat = torch.arange(pred_len_cat, device=device).unsqueeze(0) < pred_seq_len_cat.unsqueeze(1)\n",
    "        seq_mask_cat = seq_mask_cat.unsqueeze(-1).float()\n",
    "        losses[\"sn_mask\"] = bce(sn_mask_hat, sn_mask).mean()\n",
    "        losses[\"sc_mask\"] = bce(sc_mask_hat, sc_mask).mean()\n",
    "        tn_mask_loss = bce(tn_mask_hat, tn_mask_sliced).mean(dim=-1)\n",
    "        losses[\"tn_mask\"] = (tn_mask_loss * seq_mask_num[..., 0]).sum() / (seq_mask_num[..., 0].sum() + epsilon)\n",
    "        tc_mask_loss = bce(tc_mask_hat, tc_mask_sliced).mean(dim=-1)\n",
    "        losses[\"tc_mask\"] = (tc_mask_loss * seq_mask_cat[..., 0]).sum() / (seq_mask_cat[..., 0].sum() + epsilon)\n",
    "        losses[\"sn\"] = (mse(sn_hat, sn) * sn_mask).sum() / (sn_mask.sum() + epsilon)\n",
    "        losses[\"sc\"] = mse(sc_hat, sc).mean()\n",
    "        losses[\"tn\"] = (mse(tn_hat, tn_sliced) * tn_mask_sliced * seq_mask_num).sum() / ((tn_mask_sliced * seq_mask_num).sum() + epsilon)\n",
    "        losses[\"tc\"] = (mse(tc_hat, tc_sliced) * seq_mask_cat).sum() / (seq_mask_cat.sum() + epsilon)\n",
    "        losses[\"un\"] = (mse(un_hat, un_sliced) * seq_mask_num).sum() / (seq_mask_num.sum() + epsilon)\n",
    "        losses[\"uc\"] = (mse(uc_hat, uc_sliced) * seq_mask_cat).sum() / (seq_mask_cat.sum() + epsilon)\n",
    "        losses[\"len_num\"] = F.mse_loss(pred_seq_len_num.float(), true_seq_len_num.float())\n",
    "        losses[\"len_cat\"] = F.mse_loss(pred_seq_len_cat.float(), true_seq_len_cat.float())\n",
    "        total_loss = (\n",
    "            losses[\"sn_mask\"] + losses[\"sc_mask\"] +\n",
    "            losses[\"tn_mask\"] + losses[\"tc_mask\"] +\n",
    "            lambda_mse * (losses[\"sn\"] + losses[\"sc\"] + losses[\"tn\"] + losses[\"tc\"] + losses[\"un\"] + losses[\"uc\"]) +\n",
    "            lambda_len * (losses[\"len_num\"] + losses[\"len_cat\"])\n",
    "        )\n",
    "        return total_loss, losses\n",
    "\n",
    "    def fit(self, train_dataloader, val_dataloader=None, epochs=20, lr=1e-3, optimizer=\"adam\", \n",
    "            lambda_mse=1.0, lambda_len=0.1, device=\"cpu\",\n",
    "            scheduler_patience=5, scheduler_factor=0.1,resume_from=None):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "        print(f\"Training on: {device}\")\n",
    "        if optimizer.lower() == \"adam\":\n",
    "            opt = optim.Adam(self.parameters(), lr=lr)\n",
    "        elif optimizer.lower() == \"rmsprop\":\n",
    "            opt = optim.RMSprop(self.parameters(), lr=lr)\n",
    "        else:\n",
    "            raise ValueError(\"Optimizer must be 'adam' or 'rmsprop'\")\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', patience=scheduler_patience, factor=scheduler_factor)\n",
    "        start_epoch = 1\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        if resume_from is not None:\n",
    "            start_epoch = self.load_checkpoint(resume_from, optimizer=opt, scheduler=scheduler)  \n",
    "\n",
    "        for epoch in range(start_epoch, epochs + 1):\n",
    "            self.train()\n",
    "            total_train_loss = 0.0\n",
    "            train_loss_components = {}\n",
    "            for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch}/{epochs} [Train]\", leave=False):\n",
    "                sn, sc, tn, tc, un, uc, sn_mask, sc_mask, tn_mask, tc_mask, true_seq_len_num, true_seq_len_cat = [\n",
    "                    x.to(device) if torch.is_tensor(x) else x for x in batch\n",
    "                ]\n",
    "                max_seq_len_num = tn.size(1)\n",
    "                max_seq_len_cat = tc.size(1)\n",
    "                (sn_hat,sc_hat,tn_hat,tc_hat,un_hat,uc_hat,sn_mask_hat,sc_mask_hat,tn_mask_hat,tc_mask_hat,pred_seq_len_num,pred_seq_len_cat) = self(\n",
    "                    sn, sc, tn, tc, un, uc, sn_mask, sc_mask, tn_mask, tc_mask, max_seq_len_num, max_seq_len_cat\n",
    "                )\n",
    "                loss, losses_dict = self.compute_loss(\n",
    "                    sn, sc, tn, tc, un, uc, sn_mask, sc_mask, tn_mask, tc_mask,\n",
    "                    sn_hat, sc_hat, sn_mask_hat, sc_mask_hat, tn_hat, tc_hat, un_hat, uc_hat,\n",
    "                    tn_mask_hat, tc_mask_hat, pred_seq_len_num, pred_seq_len_cat,\n",
    "                    true_seq_len_num, true_seq_len_cat, lambda_mse=lambda_mse, lambda_len=lambda_len\n",
    "                )\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                total_train_loss += loss.item()\n",
    "                for k, v in losses_dict.items():\n",
    "                    train_loss_components[k] = train_loss_components.get(k, 0.0) + v.item()\n",
    "            avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "            avg_train_components = {k: v / len(train_dataloader) for k, v in train_loss_components.items()}\n",
    "            avg_val_loss = None\n",
    "            val_loss_components = {}\n",
    "            if val_dataloader is not None:\n",
    "                self.eval()\n",
    "                total_val_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for batch in tqdm(val_dataloader, desc=f\"Epoch {epoch}/{epochs} [Val]\", leave=False):\n",
    "                        sn, sc, tn, tc, un, uc, sn_mask, sc_mask, tn_mask, tc_mask, true_seq_len_num, true_seq_len_cat = [\n",
    "                            x.to(device) if torch.is_tensor(x) else x for x in batch\n",
    "                        ]\n",
    "                        max_seq_len_num = tn.size(1)\n",
    "                        max_seq_len_cat = tc.size(1)\n",
    "                        (sn_hat,sc_hat,tn_hat,tc_hat,un_hat,uc_hat,sn_mask_hat,sc_mask_hat,tn_mask_hat,tc_mask_hat,pred_seq_len_num,pred_seq_len_cat) = self(\n",
    "                            sn, sc, tn, tc, un, uc, sn_mask, sc_mask, tn_mask, tc_mask, max_seq_len_num, max_seq_len_cat)\n",
    "                        loss, losses_dict = self.compute_loss(\n",
    "                            sn, sc, tn, tc, un, uc, sn_mask, sc_mask, tn_mask, tc_mask,\n",
    "                            sn_hat, sc_hat, sn_mask_hat, sc_mask_hat, tn_hat, tc_hat, un_hat, uc_hat,\n",
    "                            tn_mask_hat, tc_mask_hat, pred_seq_len_num, pred_seq_len_cat,\n",
    "                            true_seq_len_num, true_seq_len_cat, lambda_mse=lambda_mse, lambda_len=lambda_len\n",
    "                        )\n",
    "                        total_val_loss += loss.item()\n",
    "                        for k, v in losses_dict.items():\n",
    "                            val_loss_components[k] = val_loss_components.get(k, 0.0) + v.item()\n",
    "                avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "                avg_val_components = {k: v / len(val_dataloader) for k, v in val_loss_components.items()}\n",
    "                scheduler.step(avg_val_loss)\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    self.save_checkpoint(opt, scheduler, epoch, filename='best_encoder_decoder_ckpt.pt')\n",
    "                    \n",
    "            if avg_val_loss is not None:\n",
    "                print(f\"Epoch {epoch}/{epochs} - Train Loss: {avg_train_loss:.7f} | Val Loss: {avg_val_loss:.7f} | Train LenNum: {avg_train_components['len_num']:.6f} | Train LenCat: {avg_train_components['len_cat']:.6f} | Val LenNum: {avg_val_components['len_num']:.6f} | Val LenCat: {avg_val_components['len_cat']:.6f}\")\n",
    "            else:\n",
    "                scheduler.step(avg_train_loss)\n",
    "                print(f\"Epoch {epoch}/{epochs} - Train Loss: {avg_train_loss:.7f} | Train LenNum: {avg_train_components['len_num']:.6f} | Train LenCat: {avg_train_components['len_cat']:.6f}\")\n",
    "\n",
    "        self.save_checkpoint(opt, scheduler, epoch)\n",
    "\n",
    "    def save_checkpoint(self, optimizer, scheduler, epoch, filename=\"encoder_decoder_ckpt.pt\"):\n",
    "        checkpoint = {\n",
    "            \"model_state\": self.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"scheduler_state\": scheduler.state_dict() if scheduler is not None else None,\n",
    "            \"epoch\": epoch\n",
    "        }\n",
    "        torch.save(checkpoint, filename)\n",
    "        print(f\"Checkpoint saved at epoch {epoch} -> {filename}\")\n",
    "\n",
    "    def load_checkpoint(self, filename=\"encoder_decoder_ckpt.pt\", optimizer=None, scheduler=None, map_location=None):\n",
    "        checkpoint = torch.load(filename, map_location=map_location)\n",
    "        self.load_state_dict(checkpoint[\"model_state\"])\n",
    "        if optimizer is not None:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        if scheduler is not None and checkpoint[\"scheduler_state\"] is not None:\n",
    "            scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\"Resumed from checkpoint {filename}, starting at epoch {start_epoch}\")\n",
    "        return start_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c937416",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:51:07.074715Z",
     "iopub.status.busy": "2025-10-17T15:51:07.074319Z",
     "iopub.status.idle": "2025-10-17T15:51:07.105346Z",
     "shell.execute_reply": "2025-10-17T15:51:07.104326Z",
     "shell.execute_reply.started": "2025-10-17T15:51:07.074681Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,encoder_state_dim,latent_dim = 256,hidden_dims=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [512,1024,2048,1024,512]\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = latent_dim\n",
    "\n",
    "        for i,hidden_dim in enumerate(hidden_dims):\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim,hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(0.2 if i < len(hidden_dims)//2 else 0.1)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        layers.extend([\n",
    "            nn.Linear(prev_dim,encoder_state_dim*2),\n",
    "            nn.BatchNorm1d(encoder_state_dim*2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(encoder_state_dim*2,encoder_state_dim)\n",
    "        ])\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,z):\n",
    "        return self.model(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78dfb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:51:07.107679Z",
     "iopub.status.busy": "2025-10-17T15:51:07.107025Z",
     "iopub.status.idle": "2025-10-17T15:51:07.133579Z",
     "shell.execute_reply": "2025-10-17T15:51:07.132484Z",
     "shell.execute_reply.started": "2025-10-17T15:51:07.107622Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,encoder_state_dim,hidden_dims = None):\n",
    "        super().__init__()\n",
    "\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [256,512,1024,2048,1024,512,256,128]\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = encoder_state_dim\n",
    "\n",
    "        for i,hidden_dim in enumerate(hidden_dims):\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim,hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(0.3 if i < len(hidden_dims)//2 else 0.2)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        layers.extend([\n",
    "            nn.Linear(prev_dim,64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64,1)\n",
    "        ])\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffac266",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:51:07.140523Z",
     "iopub.status.busy": "2025-10-17T15:51:07.137855Z",
     "iopub.status.idle": "2025-10-17T15:51:07.163354Z",
     "shell.execute_reply": "2025-10-17T15:51:07.161507Z",
     "shell.execute_reply.started": "2025-10-17T15:51:07.140468Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_mmd(x, y, sigma=None):\n",
    "    if isinstance(x, list):\n",
    "        x = torch.stack(x)\n",
    "    if isinstance(y, list):\n",
    "        y = torch.stack(y)\n",
    "\n",
    "    x, y = x.to(torch.float32), y.to(torch.float32)\n",
    "\n",
    "    combined = torch.cat([x, y], dim=0)\n",
    "    mean = combined.mean(dim=0, keepdim=True)\n",
    "    std = combined.std(dim=0, keepdim=True) + 1e-6\n",
    "    x_norm = (x - mean) / std\n",
    "    y_norm = (y - mean) / std\n",
    "    if sigma is None:\n",
    "        xy = torch.cat([x_norm, y_norm], dim=0)\n",
    "        dists = torch.cdist(xy, xy, p=2)\n",
    "        sigma = torch.median(dists).item()\n",
    "        if sigma == 0:\n",
    "            sigma = 1.0\n",
    "\n",
    "    def gaussian_kernel(a, b, sigma):\n",
    "        dist_sq = torch.cdist(a, b, p=2) ** 2\n",
    "        return torch.exp(-dist_sq / (2 * sigma ** 2))\n",
    "\n",
    "    k_xx = gaussian_kernel(x_norm, x_norm, sigma)\n",
    "    k_yy = gaussian_kernel(y_norm, y_norm, sigma)\n",
    "    k_xy = gaussian_kernel(x_norm, y_norm, sigma)\n",
    "\n",
    "    mmd = k_xx.mean() + k_yy.mean() - 2 * k_xy.mean()\n",
    "    return mmd.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527cc2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:51:07.165336Z",
     "iopub.status.busy": "2025-10-17T15:51:07.164997Z",
     "iopub.status.idle": "2025-10-17T15:51:07.208912Z",
     "shell.execute_reply": "2025-10-17T15:51:07.207494Z",
     "shell.execute_reply.started": "2025-10-17T15:51:07.165310Z"
    }
   },
   "outputs": [],
   "source": [
    "class WGANGP:\n",
    "    def __init__(self, encoder_state_dim, latent_dim=128,\n",
    "                 generator_hidden_dims=None, discriminator_hidden_dims=None,\n",
    "                 lr_generator=1e-4, lr_discriminator=1e-4,\n",
    "                 lambda_gp=10.0, n_critic=5, device=None,\n",
    "                 plateau_factor=0.5, plateau_patience=10):\n",
    "\n",
    "        self.encoder_state_dim = encoder_state_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lambda_gp = lambda_gp\n",
    "        self.n_critic = n_critic\n",
    "        self.device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "\n",
    "        self.generator = Generator(encoder_state_dim, latent_dim, generator_hidden_dims).to(self.device)\n",
    "        self.discriminator = Discriminator(encoder_state_dim, discriminator_hidden_dims).to(self.device)\n",
    "\n",
    "        self.optimizer_G = optim.Adam(self.generator.parameters(), lr=lr_generator, betas=(0.5, 0.9))\n",
    "        self.optimizer_D = optim.Adam(self.discriminator.parameters(), lr=lr_discriminator, betas=(0.5, 0.9))\n",
    "\n",
    "        # ReduceLROnPlateau scheduler\n",
    "        self.scheduler_G = ReduceLROnPlateau(self.optimizer_G, mode='min', factor=plateau_factor,\n",
    "                                             patience=plateau_patience)\n",
    "        self.scheduler_D = ReduceLROnPlateau(self.optimizer_D, mode='min', factor=plateau_factor,\n",
    "                                             patience=plateau_patience)\n",
    "\n",
    "        self.start_epoch = 1\n",
    "        self.best_mmd = float(\"inf\")\n",
    "\n",
    "    def gradient_penalty(self, real_samples, fake_samples):\n",
    "        real_samples = real_samples.to(self.device).float()\n",
    "        fake_samples = fake_samples.to(self.device).float()\n",
    "        batch_size = real_samples.size(0)\n",
    "        epsilon = torch.rand(batch_size, 1, device=self.device).expand_as(real_samples)\n",
    "        interpolated = epsilon * real_samples + (1 - epsilon) * fake_samples.detach()\n",
    "        interpolated.requires_grad_(True)\n",
    "\n",
    "        d_interpolated = self.discriminator(interpolated)\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolated,\n",
    "            inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(d_interpolated, device=self.device),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        gradient_penalty = self.lambda_gp * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "\n",
    "    def discriminator_loss(self, real_samples, fake_samples):\n",
    "        d_real = self.discriminator(real_samples)\n",
    "        d_fake = self.discriminator(fake_samples.detach())\n",
    "        wasserstein_distance = d_real.mean() - d_fake.mean()\n",
    "        gp = self.gradient_penalty(real_samples, fake_samples)\n",
    "        d_loss = -wasserstein_distance + gp\n",
    "        return d_loss, wasserstein_distance, gp\n",
    "\n",
    "    def generator_loss(self, fake_samples):\n",
    "        d_fake = self.discriminator(fake_samples)\n",
    "        g_loss = -d_fake.mean()\n",
    "        return g_loss\n",
    "\n",
    "    def generate_samples(self, batch_size):\n",
    "        z = torch.randn(batch_size, self.latent_dim, device=self.device)\n",
    "        fake_samples = self.generator(z).float()\n",
    "        return fake_samples\n",
    "\n",
    "    def fit(self, train_dataloader, epochs=100, resume_from=None, \n",
    "            val_dataloader=None, verbose=True):\n",
    "\n",
    "        if resume_from:\n",
    "            self.load_checkpoint(resume_from)\n",
    "            print(f\"Resumed training from checkpoint: {resume_from}\")\n",
    "\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "\n",
    "        history = {\n",
    "            \"train_d_loss\": [], \"train_g_loss\": [], \"train_wd\": [], \"train_gp\": [],\n",
    "            \"val_g_loss\": [], \"val_wd\": [], \"val_mmd\": []\n",
    "        }\n",
    "\n",
    "        for epoch in range(self.start_epoch, epochs + 1):\n",
    "            epoch_d_loss = epoch_g_loss = epoch_wd = epoch_gp = 0.0\n",
    "            batches = 0\n",
    "            progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch}/{epochs}') if verbose else train_dataloader\n",
    "\n",
    "            for i, (real_samples,) in enumerate(progress_bar):\n",
    "                batches += 1\n",
    "                real_samples = real_samples.to(self.device).float()\n",
    "                bsz = real_samples.size(0)\n",
    "\n",
    "                # ---- Train Discriminator ----\n",
    "                self.optimizer_D.zero_grad()\n",
    "                fake_samples = self.generate_samples(bsz)\n",
    "                d_loss, wd, gp = self.discriminator_loss(real_samples, fake_samples)\n",
    "                d_loss.backward()\n",
    "                self.optimizer_D.step()\n",
    "\n",
    "                epoch_d_loss += d_loss.item()\n",
    "                epoch_wd += wd.item()\n",
    "                epoch_gp += gp.item()\n",
    "\n",
    "                # ---- Train Generator every n_critic ----\n",
    "                if i % self.n_critic == 0:\n",
    "                    self.optimizer_G.zero_grad()\n",
    "                    fake_samples = self.generate_samples(bsz)\n",
    "                    g_loss = self.generator_loss(fake_samples)\n",
    "                    g_loss.backward()\n",
    "                    self.optimizer_G.step()\n",
    "                    epoch_g_loss += g_loss.item()\n",
    "\n",
    "            avg_d_loss = epoch_d_loss / batches\n",
    "            avg_g_loss = epoch_g_loss / max(1, (batches // self.n_critic))\n",
    "            avg_wd = epoch_wd / batches\n",
    "            avg_gp = epoch_gp / batches\n",
    "\n",
    "            history[\"train_d_loss\"].append(avg_d_loss)\n",
    "            history[\"train_g_loss\"].append(avg_g_loss)\n",
    "            history[\"train_wd\"].append(avg_wd)\n",
    "            history[\"train_gp\"].append(avg_gp)\n",
    "\n",
    "            # ---- Validation ----\n",
    "            if val_dataloader is not None:\n",
    "                self.generator.eval()\n",
    "                self.discriminator.eval()\n",
    "                real_embeddings, fake_embeddings = [], []\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for real_samples, in val_dataloader:\n",
    "                        real_samples = real_samples.to(self.device).float()\n",
    "                        fake_samples = self.generate_samples(real_samples.size(0))\n",
    "                        real_embeddings.append(real_samples)\n",
    "                        fake_embeddings.append(fake_samples)\n",
    "\n",
    "                real_embeddings = torch.cat(real_embeddings, dim=0)\n",
    "                fake_embeddings = torch.cat(fake_embeddings, dim=0)\n",
    "                current_mmd = compute_mmd(real_embeddings, fake_embeddings)\n",
    "                history[\"val_mmd\"].append(current_mmd)\n",
    "\n",
    "                val_g_loss = val_wd = 0.0\n",
    "                val_batches = 0\n",
    "                with torch.no_grad():\n",
    "                    for real_samples, in val_dataloader:\n",
    "                        val_batches += 1\n",
    "                        real_samples = real_samples.to(self.device).float()\n",
    "                        bsz = real_samples.size(0)\n",
    "                        fake_samples = self.generate_samples(bsz)\n",
    "\n",
    "                        d_real = self.discriminator(real_samples)\n",
    "                        d_fake = self.discriminator(fake_samples)\n",
    "\n",
    "                        wd = d_real.mean() - d_fake.mean()\n",
    "                        g_loss = self.generator_loss(fake_samples)\n",
    "                        val_g_loss += g_loss.item()\n",
    "                        val_wd += wd.item()\n",
    "\n",
    "                avg_val_g_loss = val_g_loss / val_batches\n",
    "                avg_val_wd = val_wd / val_batches\n",
    "\n",
    "                history[\"val_g_loss\"].append(avg_val_g_loss)\n",
    "                history[\"val_wd\"].append(avg_val_wd)\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"[Epoch {epoch}] Train D: {avg_d_loss:.7f}, G: {avg_g_loss:.7f}, WD: {avg_wd:.7f}, GP: {avg_gp:.7f} | \"\n",
    "                          f\"Val WD: {avg_val_wd:.7f}, MMD: {current_mmd:.7f}\")\n",
    "\n",
    "                # Save best model\n",
    "                if current_mmd < self.best_mmd:\n",
    "                    self.best_mmd = current_mmd\n",
    "                    self.save_checkpoint(\"best_gan.pt\", epoch, history, is_best=True)\n",
    "\n",
    "                self.generator.train()\n",
    "                self.discriminator.train()\n",
    "\n",
    "                # ---- Step scheduler using validation MMD ----\n",
    "                self.scheduler_G.step(current_mmd)\n",
    "                self.scheduler_D.step(current_mmd)\n",
    "\n",
    "            else:\n",
    "                if avg_wd > getattr(self, \"best_wd\", float(\"-inf\")):\n",
    "                    self.best_wd = avg_wd\n",
    "                    self.save_checkpoint(\"best_gan.pt\", epoch, history, is_best=True)\n",
    "\n",
    "        self.save_checkpoint(\"final_gan.pt\", epoch, history)\n",
    "        print(\"Training completed!\")\n",
    "        return history\n",
    "\n",
    "    def save_checkpoint(self, filename, epoch, history, is_best=False):\n",
    "        state = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"generator_state\": self.generator.state_dict(),\n",
    "            \"discriminator_state\": self.discriminator.state_dict(),\n",
    "            \"optimizer_G\": self.optimizer_G.state_dict(),\n",
    "            \"optimizer_D\": self.optimizer_D.state_dict(),\n",
    "            \"latent_dim\": self.latent_dim,\n",
    "            \"encoder_state_dim\": self.encoder_state_dim,\n",
    "            \"best_mmd\": self.best_mmd,\n",
    "            \"history\": history\n",
    "        }\n",
    "        torch.save(state, filename)\n",
    "        if is_best:\n",
    "            print(f\"Best model saved at {filename} (MMD: {self.best_mmd:.7f})\")\n",
    "        else:\n",
    "            print(f\"Checkpoint saved at {filename}\")\n",
    "\n",
    "    def load_checkpoint(self, filename, map_location=None):\n",
    "        checkpoint = torch.load(filename, map_location=map_location or self.device)\n",
    "        self.generator.load_state_dict(checkpoint[\"generator_state\"])\n",
    "        self.discriminator.load_state_dict(checkpoint[\"discriminator_state\"])\n",
    "        self.optimizer_G.load_state_dict(checkpoint[\"optimizer_G\"])\n",
    "        self.optimizer_D.load_state_dict(checkpoint[\"optimizer_D\"])\n",
    "        self.generator.to(self.device)\n",
    "        self.discriminator.to(self.device)\n",
    "        self.start_epoch = checkpoint[\"epoch\"]\n",
    "        self.best_mmd = checkpoint.get(\"best_mmd\", float(\"inf\"))\n",
    "        print(f\"Checkpoint loaded: {filename} (resuming at epoch {self.start_epoch})\")\n",
    "        return checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d8146e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:51:07.211259Z",
     "iopub.status.busy": "2025-10-17T15:51:07.210801Z",
     "iopub.status.idle": "2025-10-17T15:51:07.255552Z",
     "shell.execute_reply": "2025-10-17T15:51:07.253745Z",
     "shell.execute_reply.started": "2025-10-17T15:51:07.211231Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_synthetic_dataset(generator, decoder, total_samples, batch_size=128, device=None):\n",
    "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    decoder.to(device).eval()\n",
    "\n",
    "    sn_list, sc_list, sn_mask_list, sc_mask_list = [], [], [], []\n",
    "    tn_list, tc_list, un_list, uc_list = [], [], [], []\n",
    "    tn_mask_list, tc_mask_list = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start in tqdm(range(0, total_samples, batch_size)):\n",
    "            current_batch = min(batch_size, total_samples - start)\n",
    "            latent_samples = generator.generate_samples(batch_size=current_batch).to(device)\n",
    "            decoded = decoder.generate_decoding(latent_samples, batch_size=current_batch)\n",
    "            sn_hat, sc_hat, sn_mask_hat, sc_mask_hat, tn_hat, tc_hat, un_hat, uc_hat, tn_mask_hat, tc_mask_hat = decoded\n",
    "\n",
    "            sn_list.append(sn_hat)\n",
    "            sc_list.append(sc_hat)\n",
    "            sn_mask_list.append(sn_mask_hat)\n",
    "            sc_mask_list.append(sc_mask_hat)\n",
    "\n",
    "            # Extend temporal lists to preserve batch separation\n",
    "            if isinstance(tn_hat, list):\n",
    "                tn_list.extend(tn_hat)\n",
    "            else:\n",
    "                tn_list.append(tn_hat)\n",
    "\n",
    "            if isinstance(tc_hat, list):\n",
    "                tc_list.extend(tc_hat)\n",
    "            else:\n",
    "                tc_list.append(tc_hat)\n",
    "\n",
    "            if isinstance(un_hat, list):\n",
    "                un_list.extend(un_hat)\n",
    "            else:\n",
    "                un_list.append(un_hat)\n",
    "\n",
    "            if isinstance(uc_hat, list):\n",
    "                uc_list.extend(uc_hat)\n",
    "            else:\n",
    "                uc_list.append(uc_hat)\n",
    "\n",
    "            if isinstance(tn_mask_hat, list):\n",
    "                tn_mask_list.extend(tn_mask_hat)\n",
    "            else:\n",
    "                tn_mask_list.append(tn_mask_hat)\n",
    "\n",
    "            if isinstance(tc_mask_hat, list):\n",
    "                tc_mask_list.extend(tc_mask_hat)\n",
    "            else:\n",
    "                tc_mask_list.append(tc_mask_hat)\n",
    "\n",
    "    return (\n",
    "        torch.cat(sn_list, dim=0),     \n",
    "        torch.cat(sc_list, dim=0),     \n",
    "        torch.cat(sn_mask_list, dim=0),\n",
    "        torch.cat(sc_mask_list, dim=0),\n",
    "        tn_list,                       \n",
    "        tc_list,                        \n",
    "        un_list,                       \n",
    "        uc_list,                       \n",
    "        tn_mask_list,                    \n",
    "        tc_mask_list                   \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46e379",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:51:07.257195Z",
     "iopub.status.busy": "2025-10-17T15:51:07.256768Z",
     "iopub.status.idle": "2025-10-17T15:51:07.289197Z",
     "shell.execute_reply": "2025-10-17T15:51:07.287426Z",
     "shell.execute_reply.started": "2025-10-17T15:51:07.257157Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8302ffc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:51:07.291782Z",
     "iopub.status.busy": "2025-10-17T15:51:07.291457Z",
     "iopub.status.idle": "2025-10-17T16:07:31.431570Z",
     "shell.execute_reply": "2025-10-17T16:07:31.429452Z",
     "shell.execute_reply.started": "2025-10-17T15:51:07.291755Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = WGANGP(encoder_state_dim=256,latent_dim=256)\n",
    "generator.load_checkpoint(filename=\"weights/best_gan.pt\",map_location=device)\n",
    "sn_dim = 1           \n",
    "sce_latent_dim = 32 \n",
    "tn_dim = 25               \n",
    "tce_latent_dim = 59         \n",
    "sc_dim = 3\n",
    "tc_dim = 6                     \n",
    "decoder = EncoderDecoder(sn_dim=sn_dim,sce_latent_dim=sce_latent_dim,tn_dim=tn_dim,\n",
    "            tce_latent_dim=tce_latent_dim,sc_dim=sc_dim,tc_dim=tc_dim,latent_dim=256)\n",
    "decoder.load_checkpoint(filename=\"weights/best_encoder_decoder_ckpt.pt\",map_location=device)\n",
    "decoder.to(device)\n",
    "sn_hat, sc_hat, sn_mask_hat, sc_mask_hat, tn_hat, tc_hat, un_hat, uc_hat, tn_mask_hat, tc_mask_hat = generate_synthetic_dataset(generator, decoder, total_samples=70000, batch_size=64, device=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6005741",
   "metadata": {},
   "source": [
    "## Generate Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7708fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:07:31.437044Z",
     "iopub.status.busy": "2025-10-17T16:07:31.436343Z",
     "iopub.status.idle": "2025-10-17T16:07:31.454900Z",
     "shell.execute_reply": "2025-10-17T16:07:31.453186Z",
     "shell.execute_reply.started": "2025-10-17T16:07:31.437012Z"
    }
   },
   "outputs": [],
   "source": [
    "def denormalize_generated_data(sn_hat, tn_hat, un_hat, sn_mask_hat, tn_mask_hat,\n",
    "                               normalizer_sn, normalizer_tn, temporal_columns, use_mask=True):\n",
    "    if isinstance(sn_hat, list):\n",
    "        sn_hat = torch.cat(sn_hat, dim=0)\n",
    "\n",
    "    sn = normalizer_sn.stochastic_renormalize(sn_hat, key=\"AGE\").tolist()\n",
    "\n",
    "    tn_flat = torch.cat(tn_hat, dim=0)\n",
    "    un_flat = torch.cat(un_hat, dim=0)\n",
    "    lengths = [x.shape[0] for x in tn_hat]\n",
    "\n",
    "    tn_denorm = torch.zeros_like(tn_flat)\n",
    "    for f, key in enumerate(tqdm(temporal_columns[1:], desc=\"Denormalizing temporal features\", mininterval=0.1, leave=False)):\n",
    "        tn_denorm[:, f] = normalizer_tn.stochastic_renormalize(tn_flat[:, f], key=key)\n",
    "\n",
    "    un_denorm = normalizer_tn.stochastic_renormalize(un_flat, key=temporal_columns[0])\n",
    "    un_time = []\n",
    "    start = 0\n",
    "    for L in tqdm(lengths, desc=\"Building cumulative time\", mininterval=0.1, leave=False):\n",
    "        end = start + L\n",
    "        un_time.append(torch.cumsum(un_denorm[start:end], dim=0))\n",
    "        start = end\n",
    "\n",
    "    if use_mask:\n",
    "        if sn_mask_hat is not None:\n",
    "            sn_mask_hat = sn_mask_hat.squeeze() if sn_mask_hat.ndim > 1 else sn_mask_hat\n",
    "            sn = [float('nan') if m == 0 else (x[0] if isinstance(x, list) else x) for x, m in zip(sn, sn_mask_hat.tolist())]\n",
    "        if tn_mask_hat is not None:\n",
    "            tn_mask_flat = torch.cat(tn_mask_hat, dim=0)\n",
    "            tn_denorm = tn_denorm.masked_fill(tn_mask_flat == 0, float('nan'))\n",
    "\n",
    "    static_df = pd.DataFrame({\n",
    "        \"AGE\": sn\n",
    "    })\n",
    "\n",
    "    temporal_df = pd.DataFrame(\n",
    "        tn_denorm.cpu().numpy(),\n",
    "        columns=temporal_columns[1:]\n",
    "    )\n",
    "    temporal_df.insert(0, \"TIME\", torch.cat(un_time).cpu().numpy())\n",
    "\n",
    "    return static_df, temporal_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b846d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:07:31.458119Z",
     "iopub.status.busy": "2025-10-17T16:07:31.457654Z",
     "iopub.status.idle": "2025-10-17T16:11:52.666392Z",
     "shell.execute_reply": "2025-10-17T16:11:52.665254Z",
     "shell.execute_reply.started": "2025-10-17T16:07:31.458079Z"
    }
   },
   "outputs": [],
   "source": [
    "normalizer_sn = StochasticNormalizer()\n",
    "normalizer_sn.load_params(\"weights/static_params.pt\")\n",
    "\n",
    "normalizer_tn = StochasticNormalizer()\n",
    "normalizer_tn.load_params(\"weights/temporal_params.pt\")\n",
    "\n",
    "temporal_columns = ['DATE', 'Body Height', 'Body Mass Index', 'Body Weight', 'Calcium',\n",
    "                   'Carbon Dioxide', 'Chloride', 'Creatinine',\n",
    "                   'DXA [T-score] Bone density', 'Diastolic Blood Pressure',\n",
    "                   'Egg white IgE Ab in Serum', 'Estimated Glomerular Filtration Rate',\n",
    "                   'FEV1/FVC', 'Glucose', 'HIV status',\n",
    "                   'Hemoglobin A1c/Hemoglobin.total in Blood',\n",
    "                   'High Density Lipoprotein Cholesterol',\n",
    "                   'Low Density Lipoprotein Cholesterol', 'Microalbumin Creatine Ratio',\n",
    "                   'Oral temperature', 'Potassium', 'Sodium', 'Systolic Blood Pressure',\n",
    "                    'Total Cholesterol', 'Triglycerides', 'Urea Nitrogen']\n",
    "\n",
    "static_df,temporal_df = denormalize_generated_data(sn_hat, tn_hat, un_hat,sn_mask_hat,tn_mask_hat,\n",
    "                                            normalizer_sn, normalizer_tn,temporal_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d6096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:11:52.673335Z",
     "iopub.status.busy": "2025-10-17T16:11:52.672984Z",
     "iopub.status.idle": "2025-10-17T16:11:52.692701Z",
     "shell.execute_reply": "2025-10-17T16:11:52.691524Z",
     "shell.execute_reply.started": "2025-10-17T16:11:52.673313Z"
    }
   },
   "outputs": [],
   "source": [
    "static_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b0274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:11:52.694097Z",
     "iopub.status.busy": "2025-10-17T16:11:52.693714Z",
     "iopub.status.idle": "2025-10-17T16:11:52.817408Z",
     "shell.execute_reply": "2025-10-17T16:11:52.816164Z",
     "shell.execute_reply.started": "2025-10-17T16:11:52.694069Z"
    }
   },
   "outputs": [],
   "source": [
    "static_df.to_csv('static_numerical.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f62ce86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:11:52.818742Z",
     "iopub.status.busy": "2025-10-17T16:11:52.818420Z",
     "iopub.status.idle": "2025-10-17T16:11:52.849657Z",
     "shell.execute_reply": "2025-10-17T16:11:52.848429Z",
     "shell.execute_reply.started": "2025-10-17T16:11:52.818716Z"
    }
   },
   "outputs": [],
   "source": [
    "temporal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17e310",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:11:52.851177Z",
     "iopub.status.busy": "2025-10-17T16:11:52.850841Z",
     "iopub.status.idle": "2025-10-17T16:12:02.115674Z",
     "shell.execute_reply": "2025-10-17T16:12:02.114343Z",
     "shell.execute_reply.started": "2025-10-17T16:11:52.851153Z"
    }
   },
   "outputs": [],
   "source": [
    "temporal_df.to_csv('temporal_numerical.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa45725",
   "metadata": {},
   "source": [
    "## Generate Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6100bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:12:02.117555Z",
     "iopub.status.busy": "2025-10-17T16:12:02.117220Z",
     "iopub.status.idle": "2025-10-17T16:12:02.124396Z",
     "shell.execute_reply": "2025-10-17T16:12:02.123046Z",
     "shell.execute_reply.started": "2025-10-17T16:12:02.117529Z"
    }
   },
   "outputs": [],
   "source": [
    "static_input_dims = [5, 2, 21]\n",
    "static_categorical_cols = [\"RACE\", \"GENDER\", \"ETHNICITY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5cf74d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:12:02.126175Z",
     "iopub.status.busy": "2025-10-17T16:12:02.125732Z",
     "iopub.status.idle": "2025-10-17T16:12:02.221990Z",
     "shell.execute_reply": "2025-10-17T16:12:02.220680Z",
     "shell.execute_reply.started": "2025-10-17T16:12:02.126136Z"
    }
   },
   "outputs": [],
   "source": [
    "static_model_instance = CategoricalAutoEncoder(static_input_dims, filename=\"weights/static_categorical_encoder_decoder.pt\")\n",
    "static_model_instance = static_model_instance.to(device)\n",
    "static_model_instance.load_model(map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a576fba2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:12:02.223727Z",
     "iopub.status.busy": "2025-10-17T16:12:02.223339Z",
     "iopub.status.idle": "2025-10-17T16:12:02.233327Z",
     "shell.execute_reply": "2025-10-17T16:12:02.232284Z",
     "shell.execute_reply.started": "2025-10-17T16:12:02.223697Z"
    }
   },
   "outputs": [],
   "source": [
    "static_manager = StaticOneHotManager()\n",
    "static_manager.load(\"weights/static_encoded.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d146f55e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:12:02.234754Z",
     "iopub.status.busy": "2025-10-17T16:12:02.234462Z",
     "iopub.status.idle": "2025-10-17T16:12:03.447398Z",
     "shell.execute_reply": "2025-10-17T16:12:03.446331Z",
     "shell.execute_reply.started": "2025-10-17T16:12:02.234732Z"
    }
   },
   "outputs": [],
   "source": [
    "_, sc_onehot = static_model_instance.decode(sc_hat, mask=sc_mask_hat, return_onehot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5cf569",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:12:03.448936Z",
     "iopub.status.busy": "2025-10-17T16:12:03.448638Z",
     "iopub.status.idle": "2025-10-17T16:12:06.487240Z",
     "shell.execute_reply": "2025-10-17T16:12:06.486173Z",
     "shell.execute_reply.started": "2025-10-17T16:12:03.448913Z"
    }
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "onehot_lists = {}\n",
    "for col_name, dim in zip(static_categorical_cols, static_input_dims):\n",
    "    end = start + dim\n",
    "    onehot_lists[col_name] = [sc_onehot[i, start:end].cpu().tolist() for i in range(sc_onehot.size(0))]\n",
    "    start = end\n",
    "\n",
    "onehot_df = pd.DataFrame(onehot_lists)\n",
    "onehot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e5187d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:12:06.488612Z",
     "iopub.status.busy": "2025-10-17T16:12:06.488309Z",
     "iopub.status.idle": "2025-10-17T16:12:07.722617Z",
     "shell.execute_reply": "2025-10-17T16:12:07.721496Z",
     "shell.execute_reply.started": "2025-10-17T16:12:06.488588Z"
    }
   },
   "outputs": [],
   "source": [
    "decoded_dict = {}\n",
    "for col_name in static_categorical_cols:\n",
    "    decoded_dict[col_name] = static_manager.inverse_transform(onehot_df[col_name], col_name)\n",
    "\n",
    "decoded_df = pd.DataFrame(decoded_dict)\n",
    "decoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc8d9a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:12:07.723928Z",
     "iopub.status.busy": "2025-10-17T16:12:07.723646Z",
     "iopub.status.idle": "2025-10-17T16:12:07.852360Z",
     "shell.execute_reply": "2025-10-17T16:12:07.851176Z",
     "shell.execute_reply.started": "2025-10-17T16:12:07.723906Z"
    }
   },
   "outputs": [],
   "source": [
    "decoded_df.to_csv('static_categorical.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b0721",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:12:07.853919Z",
     "iopub.status.busy": "2025-10-17T16:12:07.853647Z",
     "iopub.status.idle": "2025-10-17T16:12:07.859322Z",
     "shell.execute_reply": "2025-10-17T16:12:07.858218Z",
     "shell.execute_reply.started": "2025-10-17T16:12:07.853899Z"
    }
   },
   "outputs": [],
   "source": [
    "temporal_input_dims = [76, 68, 126, 28, 99, 80]\n",
    "temporal_categorical_cols = ['CAREPLAN', 'REASON', 'CONDITIONS', 'ENCOUNTER_TYPE',\n",
    "       'MEDICINE', 'PROCEDURES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b454b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:12:07.860699Z",
     "iopub.status.busy": "2025-10-17T16:12:07.860332Z",
     "iopub.status.idle": "2025-10-17T16:12:08.101484Z",
     "shell.execute_reply": "2025-10-17T16:12:08.100457Z",
     "shell.execute_reply.started": "2025-10-17T16:12:07.860675Z"
    }
   },
   "outputs": [],
   "source": [
    "temporal_model_instance = CategoricalAutoEncoder(temporal_input_dims, filename=\"weights/temporal_categorical_encoder_decoder.pt\")\n",
    "temporal_model_instance = temporal_model_instance.to(device)\n",
    "temporal_model_instance.load_model(map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d11412f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:43:06.878119Z",
     "iopub.status.busy": "2025-10-17T16:43:06.877736Z",
     "iopub.status.idle": "2025-10-17T16:43:06.887533Z",
     "shell.execute_reply": "2025-10-17T16:43:06.886415Z",
     "shell.execute_reply.started": "2025-10-17T16:43:06.878081Z"
    }
   },
   "outputs": [],
   "source": [
    "temporal_manager = TemporalOneHotManager()\n",
    "temporal_manager.load(\"weights/temporal_encoded.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6f756c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:12:08.115347Z",
     "iopub.status.busy": "2025-10-17T16:12:08.115066Z",
     "iopub.status.idle": "2025-10-17T16:14:24.192466Z",
     "shell.execute_reply": "2025-10-17T16:14:24.191107Z",
     "shell.execute_reply.started": "2025-10-17T16:12:08.115326Z"
    }
   },
   "outputs": [],
   "source": [
    "tc_hat_tensor = torch.cat(tc_hat, dim=0).to(device)\n",
    "tc_mask_hat_tensor = torch.cat(tc_mask_hat, dim=0).to(device)\n",
    "preds, recon = temporal_model_instance.decode(tc_hat_tensor, mask=tc_mask_hat_tensor, return_onehot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee0d08-92f8-4fbe-acbd-768a51a821f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:14:24.193888Z",
     "iopub.status.busy": "2025-10-17T16:14:24.193517Z",
     "iopub.status.idle": "2025-10-17T16:33:01.982733Z",
     "shell.execute_reply": "2025-10-17T16:33:01.981508Z",
     "shell.execute_reply.started": "2025-10-17T16:14:24.193853Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os, gc\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "SAVE_DIR = \"onehot_parts\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "for batch_idx, chunk in enumerate(tqdm(torch.split(recon, BATCH_SIZE), desc=\"Processing batches\")):\n",
    "    start = 0\n",
    "    batch_dict = {}\n",
    "    for col_name, dim in zip(temporal_categorical_cols, temporal_input_dims):\n",
    "        end = start + dim\n",
    "        col_array = chunk[:, start:end].numpy()\n",
    "        col_int = col_array.argmax(axis=1)\n",
    "        batch_dict[col_name] = col_int\n",
    "        start = end\n",
    "    batch_df = pd.DataFrame(batch_dict)\n",
    "    batch_df.to_parquet(f\"{SAVE_DIR}/onehot_batch_{batch_idx:04d}.parquet\", index=False)\n",
    "    del batch_df, batch_dict, chunk, col_array, col_int\n",
    "    gc.collect()\n",
    "\n",
    "import glob\n",
    "files = sorted(glob.glob(f\"{SAVE_DIR}/onehot_batch_*.parquet\"))\n",
    "onehot_df = pd.concat((pd.read_parquet(f) for f in files), ignore_index=True)\n",
    "print(\"Done. One-hot DataFrame shape:\", onehot_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebcc3f2-1919-442c-97b6-046bae981c1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:33:25.288665Z",
     "iopub.status.busy": "2025-10-17T16:33:25.288273Z",
     "iopub.status.idle": "2025-10-17T16:33:25.303500Z",
     "shell.execute_reply": "2025-10-17T16:33:25.302315Z",
     "shell.execute_reply.started": "2025-10-17T16:33:25.288639Z"
    }
   },
   "outputs": [],
   "source": [
    "onehot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90d6534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:43:12.263260Z",
     "iopub.status.busy": "2025-10-17T16:43:12.262898Z",
     "iopub.status.idle": "2025-10-17T16:43:19.357904Z",
     "shell.execute_reply": "2025-10-17T16:43:19.355361Z",
     "shell.execute_reply.started": "2025-10-17T16:43:12.263236Z"
    }
   },
   "outputs": [],
   "source": [
    "decoded_temporal = pd.DataFrame()\n",
    "for col in temporal_categorical_cols:\n",
    "    decoded_temporal[col] = temporal_manager.inverse_transform2(onehot_df[col], col)\n",
    "decoded_temporal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e97ede",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:45:13.820636Z",
     "iopub.status.busy": "2025-10-17T16:45:13.820055Z",
     "iopub.status.idle": "2025-10-17T16:45:13.835314Z",
     "shell.execute_reply": "2025-10-17T16:45:13.832829Z",
     "shell.execute_reply.started": "2025-10-17T16:45:13.820598Z"
    }
   },
   "outputs": [],
   "source": [
    "def denormalize_uc(uc_hat, normalizer_uc):\n",
    "    if isinstance(uc_hat, list):\n",
    "        lengths = [x.shape[0] for x in uc_hat]\n",
    "        uc_flat = torch.cat(uc_hat, dim=0)\n",
    "    else:\n",
    "        lengths = [uc_hat.shape[0]]\n",
    "        uc_flat = uc_hat\n",
    "    uc_denorm = normalizer_uc.stochastic_renormalize(uc_flat, key='DATE')\n",
    "    uc_time_list = []\n",
    "    start = 0\n",
    "    for L in lengths:\n",
    "        end = start + L\n",
    "        uc_time_list.append(torch.cumsum(uc_denorm[start:end], dim=0))\n",
    "        start = end\n",
    "    uc_time_flat = torch.cat(uc_time_list, dim=0)\n",
    "\n",
    "    return uc_time_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bbf920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:45:16.620221Z",
     "iopub.status.busy": "2025-10-17T16:45:16.619770Z",
     "iopub.status.idle": "2025-10-17T16:46:30.631699Z",
     "shell.execute_reply": "2025-10-17T16:46:30.630541Z",
     "shell.execute_reply.started": "2025-10-17T16:45:16.620193Z"
    }
   },
   "outputs": [],
   "source": [
    "normalizer_uc = StochasticNormalizer()\n",
    "normalizer_uc.load_params(\"weights/categorical_times_params.pt\")\n",
    "uc_time = denormalize_uc(uc_hat,normalizer_uc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a51366",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:46:30.638624Z",
     "iopub.status.busy": "2025-10-17T16:46:30.638213Z",
     "iopub.status.idle": "2025-10-17T16:46:30.658706Z",
     "shell.execute_reply": "2025-10-17T16:46:30.657561Z",
     "shell.execute_reply.started": "2025-10-17T16:46:30.638593Z"
    }
   },
   "outputs": [],
   "source": [
    "decoded_temporal.insert(0, \"DATE\", uc_time.cpu().numpy())\n",
    "decoded_temporal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daa56d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T16:49:08.489320Z",
     "iopub.status.busy": "2025-10-17T16:49:08.488906Z",
     "iopub.status.idle": "2025-10-17T16:49:17.095597Z",
     "shell.execute_reply": "2025-10-17T16:49:17.094342Z",
     "shell.execute_reply.started": "2025-10-17T16:49:08.489296Z"
    }
   },
   "outputs": [],
   "source": [
    "decoded_temporal.to_csv('temporal_categorical.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a514d-143c-47cc-8e6f-8d3dc8c411e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8514692,
     "sourceId": 13415832,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
